{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ce8fade90e349b58530887b4f9d8466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_622f0937c8ee45d28237c599e643036d",
              "IPY_MODEL_a6fc04b6810b4a8589ecd9c102449e97"
            ],
            "layout": "IPY_MODEL_4812536113004e28894b7429e1d2ba18"
          }
        },
        "622f0937c8ee45d28237c599e643036d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ca28b4cad3494788003cc319d281a5",
            "placeholder": "​",
            "style": "IPY_MODEL_c8cd2f3e2738497096f5511f50b409b9",
            "value": "0.855 MB of 0.855 MB uploaded\r"
          }
        },
        "a6fc04b6810b4a8589ecd9c102449e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99eec3cdaa30452ebdc23e3b3d499914",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b50dff2dff4e48f6be1967df6fb3aeab",
            "value": 1
          }
        },
        "4812536113004e28894b7429e1d2ba18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ca28b4cad3494788003cc319d281a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cd2f3e2738497096f5511f50b409b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99eec3cdaa30452ebdc23e3b3d499914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50dff2dff4e48f6be1967df6fb3aeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Cursive Transformer\n",
        "Sam Greydanus | 2024"
      ],
      "metadata": {
        "id": "y3v2biu1Ttg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install wandb\n",
        "! wandb login"
      ],
      "metadata": {
        "id": "0QvXUjGZHl7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbd9ba2-45f3-4688-c364-61b7bdc2e6a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.2/300.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "from scipy.ndimage import rotate\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "import os, sys, time, math, argparse, io, copy, json, pdb\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)"
      ],
      "metadata": {
        "id": "g0WRHSM_Kksp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d79c99f-9cc3-4496-a217-04f422f23411"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and Tokenization"
      ],
      "metadata": {
        "id": "FxM2-WQjBJ8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_parse_data(min_ascii_length=4):\n",
        "    uploaded = files.upload()\n",
        "    file_content = next(iter(uploaded.values()))\n",
        "    data = json.loads(file_content.decode('utf-8'))\n",
        "    for i in range(len(data)):\n",
        "      data[i]['points'] = np.array(data[i]['points'])\n",
        "    data = [d for d in data if len(d['metadata']['asciiSequence']) > min_ascii_length]\n",
        "    return data\n",
        "\n",
        "# data = load_and_parse_data()\n",
        "# print(len(data))\n",
        "# min_ascii_length=4\n",
        "# data_ = [d for d in data if len(d['metadata']['asciiSequence']) > min_ascii_length]\n",
        "# print(len(data_))"
      ],
      "metadata": {
        "id": "kfFC9VAC3oPz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zriqrHLX1QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OobnYMcDKf78"
      },
      "outputs": [],
      "source": [
        "def plot_strokes(stroke, title, fig=None, ax=None):\n",
        "    \"\"\"Plot a single stroke\"\"\"\n",
        "    if fig is None or ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 2))\n",
        "\n",
        "    # Separate strokes based on pen lifts\n",
        "    strokes = []\n",
        "    current_stroke = []\n",
        "    for point in stroke:\n",
        "        if point[2] == 1:  # Pen is down\n",
        "            current_stroke.append(point)\n",
        "        else:  # Pen is up\n",
        "            if current_stroke:\n",
        "                strokes.append(current_stroke)\n",
        "                current_stroke = []\n",
        "    if current_stroke:\n",
        "        strokes.append(current_stroke)\n",
        "\n",
        "    # Plot each stroke\n",
        "    for stroke in strokes:\n",
        "        x, y = zip(*[(p[0], 1 - p[1]) for p in stroke])  # Invert y-axis\n",
        "        ax.plot(x, y, 'b-')\n",
        "\n",
        "    ax.set_aspect('equal') ; ax.set_title(title)\n",
        "\n",
        "    if fig is None:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def decompose_offsets(offsets, eps=1e-8):\n",
        "    magnitudes = np.linalg.norm(offsets[:, :2], axis=1, keepdims=True)  # Calculate magnitudes of vectors\n",
        "    unit_vectors = np.where(magnitudes == 0, np.zeros_like(offsets[:, :2]), offsets[:, :2] / (eps+magnitudes))  # Avoid division by zero\n",
        "    new_format = np.hstack((unit_vectors, magnitudes, offsets[:, 2:3]))  # Concatenate unit vectors, magnitudes and pen_down flags\n",
        "    return new_format\n",
        "\n",
        "def reconstruct_offsets(decomposed_data):\n",
        "    unit_vectors = decomposed_data[:, :2]\n",
        "    magnitudes = decomposed_data[:, 2:3]\n",
        "    pen_down = decomposed_data[:, 3:4]\n",
        "    reconstructed_offsets = unit_vectors * magnitudes  # Multiply unit vector components by magnitudes\n",
        "    reconstructed_data = np.hstack((reconstructed_offsets, pen_down))  # Concatenate the reconstructed offsets with pen_down flags\n",
        "    return reconstructed_data\n",
        "\n",
        "\n",
        "def preprocess_stroke_data(entry):\n",
        "    points = entry['points']\n",
        "    aspect_ratio = entry['metadata']['aspectRatio']\n",
        "    points[:, 0] *= aspect_ratio  # Scale x by aspect ratio\n",
        "    points[:, 0] -= points[0, 0]\n",
        "    return points\n",
        "\n",
        "def strokes_to_offsets(points):\n",
        "    # Calculate differences (dx, dy), not considering pen_down\n",
        "    offsets = np.zeros_like(points)\n",
        "    offsets[1:, 0:2] = np.diff(points[:, 0:2], axis=0)  # Compute dx, dy\n",
        "    offsets[:, 2] = points[:, 2]  # Copy pen_down directly\n",
        "\n",
        "    # Decouple direction from magnitude (this will help with tokenization)\n",
        "    offsets_dec = decompose_offsets(offsets)\n",
        "    return offsets_dec\n",
        "\n",
        "def offsets_to_strokes(offsets_dec):\n",
        "    # Calculate cumulative sums to get absolute positions\n",
        "    offsets = reconstruct_offsets(offsets_dec)\n",
        "\n",
        "    absolute_coords = np.cumsum(offsets[:, :2], axis=0)\n",
        "    stroke_data = np.hstack((absolute_coords, offsets[:, 2:3]))\n",
        "    return stroke_data\n",
        "\n",
        "def horizontal_shear(stroke, shear_range=(-0.4, 0.4)):\n",
        "    shear_factor = np.random.uniform(*shear_range)\n",
        "    shear_matrix = np.array([\n",
        "        [1, shear_factor],\n",
        "        [0, 1]])\n",
        "    stroke[:, :2] = np.dot(stroke[:, :2], shear_matrix.T)\n",
        "    return stroke\n",
        "\n",
        "def remove_random_points(stroke, remove_percentage=0.03):\n",
        "    num_points = np.random.randint(len(stroke))\n",
        "    num_remove = int(num_points * remove_percentage)\n",
        "    indices = np.random.choice(range(1, num_points - 1), num_remove, replace=False).astype(np.int32)\n",
        "    return np.delete(stroke, indices, axis=0)\n",
        "\n",
        "\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, strokes, texts, chars, max_seq_length=None, max_text_length=None, name='', augment=False):\n",
        "        self.name = name\n",
        "        self.strokes = strokes  # List of Nx4 arrays, each representing a cursive sentence\n",
        "        self.texts = texts  # List of corresponding text strings\n",
        "        self.chars = chars  # String of all possible characters\n",
        "        self.augment = augment\n",
        "\n",
        "        self.dx_bins = np.linspace(-1, 1, 71)  # 200 bins\n",
        "        self.dy_bins = np.linspace(-1, 1, 71)  # 200 bins\n",
        "\n",
        "        # Modify mag_bins to incorporate pen_down information\n",
        "        mag_bins_pen_down = np.concatenate([\n",
        "            np.asarray([0]),\n",
        "            np.linspace(0.005, 0.050, 40),  # Close around 0.01, 30 bins\n",
        "            np.geomspace(0.051, 4, 41)[:-1]  # 150 exponential bins\n",
        "        ])\n",
        "        mag_bins_pen_up = mag_bins_pen_down + max(mag_bins_pen_down) + 1  # Offset for pen-up states\n",
        "        self.mag_bins = np.concatenate([mag_bins_pen_down, mag_bins_pen_up])\n",
        "\n",
        "        self.feature_sizes = [len(self.dx_bins), len(self.dy_bins), len(self.mag_bins)]\n",
        "        self.cumulative_sizes = np.cumsum([0] + self.feature_sizes)\n",
        "\n",
        "        # Add special tokens for strokes\n",
        "        self.PAD_TOKEN = sum(self.feature_sizes)\n",
        "        self.END_TOKEN = sum(self.feature_sizes) + 1\n",
        "\n",
        "        # Character tokenization\n",
        "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:s for s,i in self.stoi.items()}\n",
        "        self.char_PAD_TOKEN = 0\n",
        "\n",
        "        if max_seq_length is None:\n",
        "            self.max_seq_length = max(len(stroke) for stroke in strokes) * 3 + 1  # *3 for unraveling, +1 for END token\n",
        "        else:\n",
        "            self.max_seq_length = max_seq_length\n",
        "\n",
        "        if max_text_length is None:\n",
        "            self.max_text_length = max(len(text) for text in texts)\n",
        "        else:\n",
        "            self.max_text_length = max_text_length\n",
        "\n",
        "    def augment_stroke(self, stroke):\n",
        "\n",
        "        stroke = remove_random_points(stroke, remove_percentage=0.03) # Drop some points\n",
        "        stroke = horizontal_shear(stroke, shear_range=(-0.3, 0.3)) # Horizontal shear\n",
        "\n",
        "        #scale = np.random.uniform(0.8, 1.25) # Random scaling\n",
        "        stroke[:, 0:1] *= np.random.uniform(0.85, 1.15)\n",
        "        stroke[:, 1:2] *= np.random.uniform(0.85, 1.15)\n",
        "\n",
        "        noise = np.random.normal(0, 0.001, stroke[:, :2].shape) # Random noise\n",
        "        stroke[:, :2] += noise\n",
        "\n",
        "        angle = np.random.uniform(-4, 4) # Random rotation\n",
        "        rad = np.deg2rad(angle)\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(rad), -np.sin(rad)],\n",
        "             [np.sin(rad), np.cos(rad)]])\n",
        "        stroke[:, :2] = np.dot(stroke[:, :2], rotation_matrix.T)\n",
        "\n",
        "        # Random starting point\n",
        "        stroke = stroke[np.random.randint(1, 8):-np.random.randint(1, 8)]\n",
        "\n",
        "        # Downsample stroke\n",
        "        stroke[1:,2:3] *= stroke[:-1,2:3] # pen_up will now always come in sets of 2+\n",
        "        stroke = stroke[::2]\n",
        "\n",
        "        return stroke\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return sum(self.feature_sizes) + 2  # +2 for PAD and END tokens\n",
        "\n",
        "    def get_char_vocab_size(self):\n",
        "        return len(self.chars) + 1  # +1 for PAD token\n",
        "\n",
        "    def get_output_length(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "    def encode_stroke(self, stroke):\n",
        "        dx_idx = np.digitize(stroke[:, 0], self.dx_bins) - 1\n",
        "        dy_idx = np.digitize(stroke[:, 1], self.dy_bins) - 1\n",
        "\n",
        "        # Encode magnitude and pen state together\n",
        "        mag_idx = np.digitize(stroke[:, 2], self.mag_bins[:len(self.mag_bins)//2]) - 1\n",
        "        mag_idx[stroke[:, 3] == 0] += len(self.mag_bins) // 2  # Offset for pen-up states\n",
        "\n",
        "        encoded = np.column_stack([\n",
        "            dx_idx + self.cumulative_sizes[0],\n",
        "            dy_idx + self.cumulative_sizes[1],\n",
        "            mag_idx + self.cumulative_sizes[2]\n",
        "        ])\n",
        "        return encoded.flatten()\n",
        "\n",
        "    def decode_stroke(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "\n",
        "        # Remove PAD and END tokens\n",
        "        ix = ix[(ix != self.PAD_TOKEN) & (ix != self.END_TOKEN)]\n",
        "\n",
        "        # Reshape the flattened array back to Nx3\n",
        "        ix = ix[:(len(ix)//3)*3]\n",
        "        ix = ix.reshape(-1, 3)\n",
        "\n",
        "        dx = self.dx_bins[(ix[:, 0] - self.cumulative_sizes[0]).clip(0, len(self.dx_bins)-1)]\n",
        "        dy = self.dy_bins[(ix[:, 1] - self.cumulative_sizes[1]).clip(0, len(self.dy_bins)-1)]\n",
        "\n",
        "        mag_idx = ix[:, 2] - self.cumulative_sizes[2]\n",
        "        pen = (mag_idx < len(self.mag_bins) // 2).astype(int)\n",
        "        mag_idx[pen == 0] -= len(self.mag_bins) // 2\n",
        "        mag = self.mag_bins[:len(self.mag_bins)//2][mag_idx.clip(0, len(self.mag_bins)//2 - 1)]\n",
        "\n",
        "        return np.column_stack([dx, dy, mag, pen])\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return torch.tensor([self.stoi.get(ch, self.char_PAD_TOKEN) for ch in text], dtype=torch.long)\n",
        "\n",
        "    def decode_text(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "        return ''.join([self.itos.get(i, '') for i in ix if i != self.char_PAD_TOKEN])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stroke = self.strokes[idx]\n",
        "        print(stroke.shape)\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        if self.augment:\n",
        "          stroke = self.augment_stroke(stroke.copy())\n",
        "\n",
        "        # Encode stroke\n",
        "        stroke_offsets = strokes_to_offsets(stroke)\n",
        "        encoded_stroke = self.encode_stroke(stroke_offsets)\n",
        "        x = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "        y = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "\n",
        "        seq_len = min(len(encoded_stroke), self.max_seq_length - 1)  # -1 to leave room for END token\n",
        "        x[:seq_len] = torch.tensor(encoded_stroke[:seq_len], dtype=torch.long)\n",
        "        x[seq_len] = self.END_TOKEN\n",
        "\n",
        "        y[:seq_len] = x[1:seq_len+1]\n",
        "        y[seq_len] = self.END_TOKEN\n",
        "\n",
        "        # Encode text (context)\n",
        "        encoded_text = self.encode_text(text)\n",
        "        c = torch.full((self.max_text_length,), self.char_PAD_TOKEN, dtype=torch.long)\n",
        "        text_len = min(len(encoded_text), self.max_text_length)\n",
        "        c[:text_len] = encoded_text[:text_len]\n",
        "\n",
        "        return x, c, y\n",
        "\n",
        "\n",
        "def create_datasets(augment=True, max_seq_length=900):\n",
        "  raw_json = load_and_parse_data()\n",
        "\n",
        "  stroke_seqs = [preprocess_stroke_data(entry) for entry in copy.deepcopy(raw_json)]\n",
        "  texts = [entry['metadata']['asciiSequence'] for entry in raw_json]\n",
        "  #chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,!?- \"\n",
        "  chars = \"abcdefghijklmnopqrstuvwxyz \"\n",
        "\n",
        "  max_seq_length = min(max_seq_length, max([len(s)*3 for s in stroke_seqs]))\n",
        "  print(f\"Number of examples in the dataset: {len(stroke_seqs)}\")\n",
        "  print(f\"Max token sequence length: {max_seq_length}\")\n",
        "  print(f\"Number of unique tokens in the stroke vocabulary: it's complicated\")\n",
        "  print(f\"Number of unique characters in the ascii vocabulary: {len(chars)}\")\n",
        "  print(\"Ascii vocabulary:\")\n",
        "  print(f'\\t\"{chars}\"')\n",
        "\n",
        "  # partition the input data into a training and the test set\n",
        "  test_set_size = min(1000, int(len(stroke_seqs) * 0.10)) # 10% of the training set, or up to 1000 examples\n",
        "  rp = torch.randperm(len(stroke_seqs)).tolist()\n",
        "\n",
        "  train_stroke_seqs = [stroke_seqs[i] for i in rp[:-test_set_size]]\n",
        "  train_texts = [texts[i] for i in rp[:-test_set_size]]\n",
        "\n",
        "  test_stroke_seqs = [stroke_seqs[i] for i in rp[-test_set_size:]]\n",
        "  test_texts = [texts[i] for i in rp[-test_set_size:]]\n",
        "  print(f\"Split up the dataset into {len(train_stroke_seqs)} training examples and {len(test_stroke_seqs)} test examples\")\n",
        "\n",
        "  # wrap in dataset objects\n",
        "  train_dataset = StrokeDataset(train_stroke_seqs, train_texts, chars, max_seq_length, name='train', augment=augment)\n",
        "  test_dataset = StrokeDataset(test_stroke_seqs, test_texts, chars, max_seq_length, name='test', augment=augment)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader:\n",
        "    \"\"\"\n",
        "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
        "    a better way in PyTorch to just create an infinite dataloader?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
        "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
        "        self.data_iter = iter(self.train_loader)\n",
        "\n",
        "    def next(self):\n",
        "        try:\n",
        "            batch = next(self.data_iter)\n",
        "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
        "            self.data_iter = iter(self.train_loader)\n",
        "            batch = next(self.data_iter)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, _ = create_datasets(augment=True, max_seq_length=3000)"
      ],
      "metadata": {
        "id": "pRH6-roo_bZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "d20ab7b3-88ad-44fb-c601-9d76975fab94"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc5b9973-3de8-4214-b5d8-42fb92db5236\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc5b9973-3de8-4214-b5d8-42fb92db5236\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 0005-sam4word.json to 0005-sam4word (5).json\n",
            "Number of examples in the dataset: 7\n",
            "Max token sequence length: 3000\n",
            "Number of unique tokens in the stroke vocabulary: it's complicated\n",
            "Number of unique characters in the ascii vocabulary: 27\n",
            "Ascii vocabulary:\n",
            "\t\"abcdefghijklmnopqrstuvwxyz \"\n",
            "Split up the dataset into 4 training examples and 3 test examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "example_ix = 1\n",
        "x1, c1, y1 = dataset[example_ix]  # Get tokenized version of the second example\n",
        "x2, c2, y2 = dataset[example_ix]\n",
        "\n",
        "o1 = dataset.decode_stroke(x1)\n",
        "r1 = offsets_to_strokes(o1)\n",
        "fig, ax = plot_strokes(r1, title='Reconstructed text (data augmentation seed 1)')\n",
        "\n",
        "o2 = dataset.decode_stroke(x2)\n",
        "r2 = offsets_to_strokes(o2)\n",
        "fig, ax = plot_strokes(r2, title='Reconstructed from tokens (data augmentation seed 2)')"
      ],
      "metadata": {
        "id": "OwL8_iYoEmSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "233b422d-6329-44ee-ba94-1c94c574b91c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1082, 3)\n",
            "(1082, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_stroke = o2#dataset.decode_stroke(x)\n",
        "decoded_text = c1 #dataset.decode_text(c)\n",
        "\n",
        "print(\"Original stroke shape:\", (1082,3))\n",
        "print(\"Encoded stroke shape:\", x1.shape)\n",
        "print(\"Encoded text shape:\", c1.shape)\n",
        "print(\"Decoded stroke shape:\", decoded_stroke.shape)\n",
        "\n",
        "print(\"\\nEncoded strokes (first 15 tokens, aka 5 pen strokes):\")\n",
        "print([v.item() for v in x1[:15]])\n",
        "print(\"\\n\\nEncoded text (first 10 tokens, aka ascii chars):\")\n",
        "print(c1[:10])\n",
        "print(\"\\nDecoded stroke (first 5 rows, aka 5 pen steps):\")\n",
        "print(decoded_stroke[:5])\n",
        "print(\"\\nDecoded text:\")\n",
        "print(dataset.decode_text(c1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqTTPtzs6ot",
        "outputId": "5b2c028e-e1d3-4ad0-ccbe-e6891d0aa645"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original stroke shape: (1082, 3)\n",
            "Encoded stroke shape: torch.Size([3000])\n",
            "Encoded text shape: torch.Size([31])\n",
            "Decoded stroke shape: (537, 4)\n",
            "\n",
            "Encoded strokes (first 15 tokens, aka 5 pen strokes):\n",
            "[35, 106, 142, 61, 83, 159, 59, 80, 170, 63, 86, 161, 63, 86, 162]\n",
            "\n",
            "\n",
            "Encoded text (first 10 tokens, aka ascii chars):\n",
            "tensor([ 1,  3, 20, 15, 18, 27, 13,  1, 19, 19])\n",
            "\n",
            "Decoded stroke (first 5 rows, aka 5 pen steps):\n",
            "[[ 0.          0.          0.          1.        ]\n",
            " [ 0.51428571 -0.85714286  0.02115385  1.        ]\n",
            " [ 0.65714286 -0.74285714  0.02115385  1.        ]\n",
            " [ 0.77142857 -0.62857143  0.03730769  1.        ]\n",
            " [ 0.8        -0.6         0.03615385  1.        ]]\n",
            "\n",
            "Decoded text:\n",
            "actor mass afford zoo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.decode_text(c)"
      ],
      "metadata": {
        "id": "GfuIYiIE5HlP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76491c99-0c46-46a0-e8b3-76febb181da7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wet orchard minor season'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save_samples(model, train_dataset)\n",
        "\n",
        "# plt.figure(dpi=200)\n",
        "# img = mpimg.imread('train_sample_1.png')\n",
        "# plt.imshow(img) ; plt.axis('off') ; plt.show()\n",
        "\n",
        "# x, c, y = train_dataset[0]\n",
        "# x[:5], c[:5], y[:5]"
      ],
      "metadata": {
        "id": "2u4lwjuX2MaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tow0IHOZ2Mcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "LXAlpngErwVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        # query projections for all heads\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # key, value projections for all heads\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        # calculate query for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # calculate key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "\n",
        "        # cross-attention; (B, nh, T, hs) x (B, nh, hs, T_ctx) -> (B, nh, T, T_ctx)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T_ctx) x (B, nh, T_ctx, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "Azf2fXB3MeRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Transformer Language Model (*exactly* as used in GPT-2)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = CrossAttention(config) # NEW\n",
        "        self.ln_3 = nn.LayerNorm(config.n_embd) # NEW\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        #x = x + self.mlpf(self.ln_2(x))\n",
        "        x = x + self.cross_attn(self.ln_2(x), context)\n",
        "        x = x + self.mlpf(self.ln_3(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wce = nn.Embedding(config.context_vocab_size, config.n_embd), # NEW\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"Number of Transformer parameters: {:.0f}\".format(n_params,))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def forward(self, idx, context, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        context_emb = self.transformer.wce(context) # context embeddings of shape (b, t_ctx, n_embd)\n",
        "\n",
        "        if self.config.ablate_cross_attention:\n",
        "          context_emb = torch.zeros_like(context_emb)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, context_emb)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = None # length of the input sequences of integers\n",
        "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
        "    context_vocab_size: int = None # size of the context vocabulary (ASCII characters)\n",
        "    context_length: int = None # maximum length of the context sequence\n",
        "    # parameters below control the sizes of each model slightly differently\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    n_ctx_head: int = 4 # number of heads for cross-attention\n",
        "    ablate_cross_attention: bool = False"
      ],
      "metadata": {
        "id": "-x74SZP_qymr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "PoVuOu7ZB3HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_string(fmt='%m%d_%H%M'):\n",
        "    return datetime.now().strftime(fmt)\n",
        "\n",
        "run_tag = \"ct_223samples_4char+\"\n",
        "\n",
        "@dataclass\n",
        "class AppConfig:\n",
        "    # system/input/output\n",
        "    input_file: str = 'names.txt'\n",
        "    work_dir: str = 'out'\n",
        "    resume: bool = False\n",
        "    sample_only: bool = False\n",
        "    num_workers: int = 1 # 4\n",
        "    max_steps: int = 30000\n",
        "    device: str = 'cuda'\n",
        "    seed: int = 3407\n",
        "\n",
        "    # sampling\n",
        "    top_k: int = -1\n",
        "\n",
        "    # model configuration\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    ablate_cross_attention: bool = False  # New flag to ablate cross-attention\n",
        "    augment: bool = True\n",
        "    max_seq_length: int = 900\n",
        "\n",
        "    # optimization\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-4\n",
        "\n",
        "    # wandb parameters\n",
        "    wandb_project: str = run_tag\n",
        "    wandb_entity: str = 'sam-greydanus'  # Set this to your wandb username or team name\n",
        "    wandb_run_name: str = f\"{get_time_string()}_{run_tag}\"\n",
        "\n",
        "args = AppConfig()\n",
        "\n",
        "# system inits\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "os.makedirs(args.work_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=args.work_dir)\n",
        "\n",
        "# init datasets\n",
        "train_dataset, test_dataset = create_datasets(augment=args.augment, max_seq_length=args.max_seq_length)\n",
        "vocab_size = train_dataset.get_vocab_size()\n",
        "block_size = train_dataset.get_output_length()\n",
        "context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "print(f\"Dataset determined that: {vocab_size=}, {block_size=}\")\n",
        "\n",
        "# init model\n",
        "config = ModelConfig(vocab_size=vocab_size,\n",
        "                     block_size=block_size,\n",
        "                     context_vocab_size=context_vocab_size,\n",
        "                     n_layer=args.n_layer, n_head=args.n_head,\n",
        "                     n_embd=args.n_embd, n_embd2=args.n_embd2,\n",
        "                     ablate_cross_attention=args.ablate_cross_attention,\n",
        "                     n_ctx_head=args.n_head,)\n",
        "model = Transformer(config)\n",
        "model.to(args.device)\n",
        "print(f\"Model #params: {sum(p.numel() for p in model.parameters())}\")\n",
        "if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n",
        "    print(\"resuming from existing model in the workdir\")\n",
        "    model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n",
        "if args.sample_only:\n",
        "    # save_samples(num=50)\n",
        "    print('This functionality is temporarily commented out')\n",
        "    sys.exit()\n",
        "\n",
        "# init optimizer and batch loader\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
        "batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)"
      ],
      "metadata": {
        "id": "exe5-vbitBJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "26869b8a-bf97-4b78-ae4b-8744053236d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b343acd3-6dba-4f1d-a5ee-161478831aed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b343acd3-6dba-4f1d-a5ee-161478831aed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset.json to dataset (11).json\n",
            "Number of examples in the dataset: 366\n",
            "Max token sequence length: 900\n",
            "Number of unique tokens in the stroke vocabulary: it's complicated\n",
            "Number of unique characters in the ascii vocabulary: 27\n",
            "Ascii vocabulary:\n",
            "\t\"abcdefghijklmnopqrstuvwxyz \"\n",
            "Split up the dataset into 330 training examples and 36 test examples\n",
            "Dataset determined that: vocab_size=306, block_size=900\n",
            "Number of Transformer parameters: 346112\n",
            "Model #params: 365696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=args.wandb_project,\n",
        "    entity=args.wandb_entity,\n",
        "    name=args.wandb_run_name,\n",
        "    config=args\n",
        ")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"n_layer\": config.n_layer,\n",
        "    \"n_head\": config.n_head,\n",
        "    \"n_embd\": config.n_embd,\n",
        "    \"learning_rate\": args.learning_rate,\n",
        "    \"weight_decay\": args.weight_decay,\n",
        "    \"batch_size\": args.batch_size,\n",
        "    \"ablate_cross_attention\": args.ablate_cross_attention,\n",
        "})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MvHVOusgH_z8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "569a4c39-d1c9-459c-89fe-146a90f3aac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240706_004256-dkt0usyu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B/runs/dkt0usyu' target=\"_blank\">0706_0042_ct_223samples_4char+</a></strong> to <a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B' target=\"_blank\">https://wandb.ai/sam-greydanus/ct_223samples_4char%2B</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B/runs/dkt0usyu' target=\"_blank\">https://wandb.ai/sam-greydanus/ct_223samples_4char%2B/runs/dkt0usyu</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample from the untrained model"
      ],
      "metadata": {
        "id": "Z_EKkV_W1tZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
        "    losses = []\n",
        "    for i, batch in enumerate(loader):\n",
        "        batch = [t.to(args.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "        logits, loss = model(X, C, Y)\n",
        "        losses.append(loss.item())\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "    mean_loss = torch.tensor(losses).mean().item()\n",
        "    model.train() # reset model back to training mode\n",
        "    return mean_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, idx, context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    steps = max(0, max_new_tokens-idx.size(1))\n",
        "    for i in range(steps):\n",
        "        # if the sequence context is growing too long we must crop it at block_size\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "        # forward the model to get the logits for the index in the sequence\n",
        "        logits, _ = model(idx_cond, context)\n",
        "        # pluck the logits at the final step and scale by desired temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop the logits to only the top k options\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        # apply softmax to convert logits to (normalized) probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # either sample from the distribution or take the most likely element\n",
        "        if do_sample:\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "        # append sampled index to the running sequence and continue\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def save_samples(model, dataset, num=2, model_device='cpu', warmup_steps=100, do_sample=False):\n",
        "    \"\"\" samples from the model and plots the decoded strokes \"\"\"\n",
        "    model_device = list(model.parameters())[0].device # hacky\n",
        "\n",
        "    stroke_seq, context = [], []\n",
        "    for i in range(num):\n",
        "      x, c, y = dataset[i]\n",
        "      stroke_seq.append(x) ; context.append(c)\n",
        "\n",
        "    X_init = torch.stack(stroke_seq).to(model_device)[:,:warmup_steps]\n",
        "    context = torch.stack(context).long().to(model_device)\n",
        "    top_k = None\n",
        "    steps = dataset.get_output_length() - 1  # -1 because we already start with the first token\n",
        "\n",
        "    X_samp = generate(model, X_init, context, steps, top_k=top_k, do_sample=do_sample).to('cpu')\n",
        "\n",
        "    for i in range(X_samp.size(0)):\n",
        "        # get the i'th row of sampled integers, as python list\n",
        "        row = X_samp[i].detach().cpu().numpy()\n",
        "        offset_samp = dataset.decode_stroke(row)\n",
        "        point_samp = offsets_to_strokes(offset_samp)\n",
        "        decoded_ascii = dataset.decode_text(context[i])\n",
        "\n",
        "        # Plot the stroke\n",
        "        fig, ax = plot_strokes(point_samp, f'Sample {i+1}: \"{decoded_ascii}\"') #plt.axis('off')\n",
        "        tag = 'sample' if do_sample else 'topk'\n",
        "        fig.savefig(f\"{dataset.name}_{tag}_{i+1}.png\")\n",
        "        wandb.log({f\"{dataset.name}_{tag}_{i+1}\": wandb.Image(f\"{dataset.name}_{tag}_{i+1}.png\")})\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved {dataset.name}_{tag}_{i+1}.png\")\n",
        "\n",
        "    print('-'*80)"
      ],
      "metadata": {
        "id": "RplUgs12B8xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "nV1ntAoG1vqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "best_loss = None\n",
        "step = 0\n",
        "while True:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # get the next batch, ship to device, and unpack it to input and target\n",
        "    batch = batch_loader.next()\n",
        "    batch = [t.to(args.device) for t in batch]\n",
        "    X, C, Y = batch\n",
        "\n",
        "    # feed into the model\n",
        "    logits, loss = model(X, C, Y)\n",
        "\n",
        "    # calculate the gradient, update the weights\n",
        "    model.zero_grad(set_to_none=True) ; loss.backward()\n",
        "    optimizer.step()\n",
        "    wandb.log({\"train_loss_step\": loss.item(), \"step\": step})\n",
        "\n",
        "    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n",
        "    if args.device.startswith('cuda'):\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # logging\n",
        "    if step % 25 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
        "\n",
        "    # evaluate the model\n",
        "    if step > 0 and step % 200 == 0:\n",
        "        train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
        "        test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n",
        "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss, \"step\": step })\n",
        "        print(f\"step {step} train loss: {train_loss:.4f} test loss: {test_loss:.4f}\")\n",
        "        # save the model to disk if it has improved\n",
        "        if best_loss is None or test_loss < best_loss:\n",
        "            out_path = os.path.join(args.work_dir, \"model.pt\")\n",
        "            print(f\"Test loss {test_loss:.4f} is the best so far, saving model to {out_path}\")\n",
        "            torch.save(model.state_dict(), out_path)\n",
        "            #wandb.save(out_path)\n",
        "            best_loss = test_loss\n",
        "\n",
        "    # sample from the model\n",
        "    if step > 0 and step % 200 == 0:\n",
        "        save_samples(model, test_dataset, num=3, do_sample=True)\n",
        "        save_samples(model, test_dataset, num=3, do_sample=False)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=True)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=False)\n",
        "\n",
        "    step += 1\n",
        "    # termination conditions\n",
        "    if args.max_steps >= 0 and step >= args.max_steps:\n",
        "        break"
      ],
      "metadata": {
        "id": "7otFYXQhr9lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f830d0a0-7c56-4ea5-a43e-e05fb0f8bb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 5.9542 | step time 176.32ms\n",
            "step 25 | loss 4.8422 | step time 148.56ms\n",
            "step 50 | loss 4.4576 | step time 148.16ms\n",
            "step 75 | loss 4.1101 | step time 147.92ms\n",
            "step 100 | loss 3.8891 | step time 147.89ms\n",
            "step 125 | loss 3.6873 | step time 148.03ms\n",
            "step 150 | loss 3.5314 | step time 147.94ms\n",
            "step 175 | loss 3.1122 | step time 147.95ms\n",
            "step 200 | loss 3.0202 | step time 148.39ms\n",
            "step 200 train loss: 2.9909 test loss: 2.9728\n",
            "Test loss 2.9728 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 225 | loss 2.7873 | step time 148.40ms\n",
            "step 250 | loss 2.6892 | step time 148.05ms\n",
            "step 275 | loss 2.5553 | step time 147.98ms\n",
            "step 300 | loss 2.4469 | step time 147.94ms\n",
            "step 325 | loss 2.4389 | step time 147.93ms\n",
            "step 350 | loss 2.3836 | step time 148.12ms\n",
            "step 375 | loss 2.4546 | step time 148.12ms\n",
            "step 400 | loss 2.3037 | step time 148.42ms\n",
            "step 400 train loss: 2.3596 test loss: 2.3548\n",
            "Test loss 2.3548 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 425 | loss 2.3128 | step time 148.37ms\n",
            "step 450 | loss 2.2912 | step time 147.94ms\n",
            "step 475 | loss 2.3408 | step time 147.85ms\n",
            "step 500 | loss 2.3162 | step time 148.27ms\n",
            "step 525 | loss 2.2307 | step time 148.15ms\n",
            "step 550 | loss 2.2442 | step time 147.94ms\n",
            "step 575 | loss 2.2978 | step time 148.03ms\n",
            "step 600 | loss 2.2567 | step time 148.48ms\n",
            "step 600 train loss: 2.2647 test loss: 2.2856\n",
            "Test loss 2.2856 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 625 | loss 2.1737 | step time 148.27ms\n",
            "step 650 | loss 2.2621 | step time 147.96ms\n",
            "step 675 | loss 2.1222 | step time 147.95ms\n",
            "step 700 | loss 2.3208 | step time 147.95ms\n",
            "step 725 | loss 2.2844 | step time 148.11ms\n",
            "step 750 | loss 2.3311 | step time 147.87ms\n",
            "step 775 | loss 2.2624 | step time 147.89ms\n",
            "step 800 | loss 2.1775 | step time 148.60ms\n",
            "step 800 train loss: 2.2297 test loss: 2.2600\n",
            "Test loss 2.2600 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 825 | loss 2.2125 | step time 148.61ms\n",
            "step 850 | loss 2.2653 | step time 148.10ms\n",
            "step 875 | loss 2.3272 | step time 147.91ms\n",
            "step 900 | loss 2.1990 | step time 147.88ms\n",
            "step 925 | loss 2.1519 | step time 147.98ms\n",
            "step 950 | loss 2.1547 | step time 147.99ms\n",
            "step 975 | loss 2.1972 | step time 148.01ms\n",
            "step 1000 | loss 2.2297 | step time 148.04ms\n",
            "step 1000 train loss: 2.2136 test loss: 2.2410\n",
            "Test loss 2.2410 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 1025 | loss 2.1875 | step time 148.13ms\n",
            "step 1050 | loss 2.1547 | step time 148.10ms\n",
            "step 1075 | loss 2.1865 | step time 147.93ms\n",
            "step 1100 | loss 2.2925 | step time 148.17ms\n",
            "step 1125 | loss 2.2392 | step time 148.00ms\n",
            "step 1150 | loss 2.1856 | step time 148.04ms\n",
            "step 1175 | loss 2.3169 | step time 148.02ms\n",
            "step 1200 | loss 2.2114 | step time 148.47ms\n",
            "step 1200 train loss: 2.2246 test loss: 2.2331\n",
            "Test loss 2.2331 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 1225 | loss 2.1719 | step time 148.34ms\n",
            "step 1250 | loss 2.1534 | step time 147.97ms\n",
            "step 1275 | loss 2.2636 | step time 147.76ms\n",
            "step 1300 | loss 2.2528 | step time 147.91ms\n",
            "step 1325 | loss 2.1931 | step time 148.06ms\n",
            "step 1350 | loss 2.1761 | step time 147.93ms\n",
            "step 1375 | loss 2.2007 | step time 148.00ms\n",
            "step 1400 | loss 2.2184 | step time 148.07ms\n",
            "step 1400 train loss: 2.1652 test loss: 2.2222\n",
            "Test loss 2.2222 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 1425 | loss 2.1217 | step time 148.74ms\n",
            "step 1450 | loss 2.1839 | step time 147.94ms\n",
            "step 1475 | loss 2.2212 | step time 148.32ms\n",
            "step 1500 | loss 2.2247 | step time 148.34ms\n",
            "step 1525 | loss 2.2235 | step time 148.06ms\n",
            "step 1550 | loss 2.1777 | step time 148.00ms\n",
            "step 1575 | loss 2.2039 | step time 148.06ms\n",
            "step 1600 | loss 2.2509 | step time 148.13ms\n",
            "step 1600 train loss: 2.1974 test loss: 2.2003\n",
            "Test loss 2.2003 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 1625 | loss 2.1449 | step time 148.72ms\n",
            "step 1650 | loss 2.2236 | step time 147.92ms\n",
            "step 1675 | loss 2.1569 | step time 147.97ms\n",
            "step 1700 | loss 2.1990 | step time 147.98ms\n",
            "step 1725 | loss 2.2118 | step time 148.00ms\n",
            "step 1750 | loss 2.1200 | step time 148.22ms\n",
            "step 1775 | loss 2.0501 | step time 148.20ms\n",
            "step 1800 | loss 2.1124 | step time 148.28ms\n",
            "step 1800 train loss: 2.1512 test loss: 2.1769\n",
            "Test loss 2.1769 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 1825 | loss 2.0903 | step time 148.37ms\n",
            "step 1850 | loss 2.1602 | step time 148.02ms\n",
            "step 1875 | loss 2.0746 | step time 147.98ms\n",
            "step 1900 | loss 2.2517 | step time 148.27ms\n",
            "step 1925 | loss 2.1914 | step time 147.95ms\n",
            "step 1950 | loss 2.2091 | step time 148.00ms\n",
            "step 1975 | loss 2.1359 | step time 148.02ms\n",
            "step 2000 | loss 2.0447 | step time 148.42ms\n",
            "step 2000 train loss: 2.0623 test loss: 2.0952\n",
            "Test loss 2.0952 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2025 | loss 1.9872 | step time 148.41ms\n",
            "step 2050 | loss 2.0946 | step time 148.06ms\n",
            "step 2075 | loss 2.0476 | step time 147.95ms\n",
            "step 2100 | loss 2.0804 | step time 148.17ms\n",
            "step 2125 | loss 2.0147 | step time 147.96ms\n",
            "step 2150 | loss 2.0358 | step time 148.01ms\n",
            "step 2175 | loss 2.0014 | step time 148.00ms\n",
            "step 2200 | loss 2.0732 | step time 148.20ms\n",
            "step 2200 train loss: 2.0198 test loss: 2.0357\n",
            "Test loss 2.0357 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2225 | loss 1.9306 | step time 147.95ms\n",
            "step 2250 | loss 2.0481 | step time 147.83ms\n",
            "step 2275 | loss 2.0322 | step time 147.91ms\n",
            "step 2300 | loss 2.0332 | step time 148.37ms\n",
            "step 2325 | loss 2.0265 | step time 148.19ms\n",
            "step 2350 | loss 1.9542 | step time 147.95ms\n",
            "step 2375 | loss 2.0256 | step time 148.08ms\n",
            "step 2400 | loss 1.9593 | step time 148.04ms\n",
            "step 2400 train loss: 1.9637 test loss: 1.9913\n",
            "Test loss 1.9913 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2425 | loss 1.9565 | step time 148.09ms\n",
            "step 2450 | loss 1.9369 | step time 147.87ms\n",
            "step 2475 | loss 1.9260 | step time 147.81ms\n",
            "step 2500 | loss 2.0378 | step time 148.40ms\n",
            "step 2525 | loss 2.0132 | step time 147.96ms\n",
            "step 2550 | loss 1.9714 | step time 148.23ms\n",
            "step 2575 | loss 1.8944 | step time 148.35ms\n",
            "step 2600 | loss 1.9661 | step time 148.12ms\n",
            "step 2600 train loss: 1.9501 test loss: 1.9665\n",
            "Test loss 1.9665 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2625 | loss 1.9321 | step time 147.89ms\n",
            "step 2650 | loss 1.8936 | step time 147.93ms\n",
            "step 2675 | loss 1.9772 | step time 148.05ms\n",
            "step 2700 | loss 1.9878 | step time 148.40ms\n",
            "step 2725 | loss 1.8987 | step time 147.82ms\n",
            "step 2750 | loss 1.9422 | step time 147.81ms\n",
            "step 2775 | loss 1.8789 | step time 147.99ms\n",
            "step 2800 | loss 1.9229 | step time 148.14ms\n",
            "step 2800 train loss: 1.9368 test loss: 1.9678\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2825 | loss 1.9552 | step time 148.10ms\n",
            "step 2850 | loss 1.9081 | step time 148.02ms\n",
            "step 2875 | loss 1.9577 | step time 147.91ms\n",
            "step 2900 | loss 1.8791 | step time 148.58ms\n",
            "step 2925 | loss 1.9397 | step time 147.95ms\n",
            "step 2950 | loss 1.8957 | step time 148.00ms\n",
            "step 2975 | loss 1.9566 | step time 148.31ms\n",
            "step 3000 | loss 1.9302 | step time 148.65ms\n",
            "step 3000 train loss: 1.9109 test loss: 1.9393\n",
            "Test loss 1.9393 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 3025 | loss 1.8750 | step time 148.16ms\n",
            "step 3050 | loss 1.9558 | step time 147.97ms\n",
            "step 3075 | loss 1.9216 | step time 148.08ms\n",
            "step 3100 | loss 1.8850 | step time 148.27ms\n",
            "step 3125 | loss 1.9793 | step time 148.00ms\n",
            "step 3150 | loss 1.8837 | step time 147.97ms\n",
            "step 3175 | loss 1.9790 | step time 148.12ms\n",
            "step 3200 | loss 1.8614 | step time 148.14ms\n",
            "step 3200 train loss: 1.8936 test loss: 1.9230\n",
            "Test loss 1.9230 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 3225 | loss 1.9698 | step time 147.93ms\n",
            "step 3250 | loss 1.8610 | step time 148.15ms\n",
            "step 3275 | loss 1.9398 | step time 148.02ms\n",
            "step 3300 | loss 1.8353 | step time 148.41ms\n",
            "step 3325 | loss 1.9279 | step time 148.26ms\n",
            "step 3350 | loss 1.9064 | step time 148.09ms\n",
            "step 3375 | loss 1.8940 | step time 148.36ms\n",
            "step 3400 | loss 1.8449 | step time 148.20ms\n",
            "step 3400 train loss: 1.8859 test loss: 1.9146\n",
            "Test loss 1.9146 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 3425 | loss 1.8934 | step time 147.94ms\n",
            "step 3450 | loss 1.8240 | step time 147.92ms\n",
            "step 3475 | loss 1.9430 | step time 148.02ms\n",
            "step 3500 | loss 1.8707 | step time 148.36ms\n",
            "step 3525 | loss 1.9577 | step time 147.96ms\n",
            "step 3550 | loss 1.9119 | step time 148.02ms\n",
            "step 3575 | loss 1.9066 | step time 148.52ms\n",
            "step 3600 | loss 1.8867 | step time 147.98ms\n",
            "step 3600 train loss: 1.8847 test loss: 1.9147\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 3625 | loss 1.8785 | step time 148.13ms\n",
            "step 3650 | loss 1.8321 | step time 147.91ms\n",
            "step 3675 | loss 1.7511 | step time 148.08ms\n",
            "step 3700 | loss 1.9067 | step time 148.04ms\n",
            "step 3725 | loss 1.8631 | step time 148.21ms\n",
            "step 3750 | loss 1.8144 | step time 147.93ms\n",
            "step 3775 | loss 1.8888 | step time 148.06ms\n",
            "step 3800 | loss 1.7845 | step time 148.07ms\n",
            "step 3800 train loss: 1.8643 test loss: 1.8934\n",
            "Test loss 1.8934 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 3825 | loss 1.8965 | step time 147.97ms\n",
            "step 3850 | loss 1.8374 | step time 148.02ms\n",
            "step 3875 | loss 1.8519 | step time 148.02ms\n",
            "step 3900 | loss 1.9531 | step time 148.10ms\n",
            "step 3925 | loss 1.8626 | step time 148.30ms\n",
            "step 3950 | loss 1.8053 | step time 148.01ms\n",
            "step 3975 | loss 1.8744 | step time 148.20ms\n",
            "step 4000 | loss 1.8379 | step time 148.08ms\n",
            "step 4000 train loss: 1.8445 test loss: 1.8814\n",
            "Test loss 1.8814 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4025 | loss 1.8376 | step time 147.98ms\n",
            "step 4050 | loss 1.8675 | step time 147.80ms\n",
            "step 4075 | loss 1.8031 | step time 147.98ms\n",
            "step 4100 | loss 1.8804 | step time 148.03ms\n",
            "step 4125 | loss 1.8983 | step time 148.05ms\n",
            "step 4150 | loss 1.8964 | step time 147.98ms\n",
            "step 4175 | loss 1.8364 | step time 148.22ms\n",
            "step 4200 | loss 1.8653 | step time 148.24ms\n",
            "step 4200 train loss: 1.8504 test loss: 1.8765\n",
            "Test loss 1.8765 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4225 | loss 1.8326 | step time 148.01ms\n",
            "step 4250 | loss 1.8611 | step time 147.96ms\n",
            "step 4275 | loss 1.8493 | step time 148.09ms\n",
            "step 4300 | loss 1.8448 | step time 148.17ms\n",
            "step 4325 | loss 1.7690 | step time 150.10ms\n",
            "step 4350 | loss 1.8788 | step time 147.92ms\n",
            "step 4375 | loss 1.8908 | step time 148.44ms\n",
            "step 4400 | loss 1.8044 | step time 148.19ms\n",
            "step 4400 train loss: 1.8381 test loss: 1.8741\n",
            "Test loss 1.8741 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4425 | loss 1.7883 | step time 147.96ms\n",
            "step 4450 | loss 1.8462 | step time 147.88ms\n",
            "step 4475 | loss 1.9179 | step time 148.08ms\n",
            "step 4500 | loss 1.8344 | step time 148.06ms\n",
            "step 4525 | loss 1.8268 | step time 147.98ms\n",
            "step 4550 | loss 1.8809 | step time 148.06ms\n",
            "step 4575 | loss 1.8104 | step time 148.39ms\n",
            "step 4600 | loss 1.8403 | step time 147.93ms\n",
            "step 4600 train loss: 1.8412 test loss: 1.8670\n",
            "Test loss 1.8670 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4625 | loss 1.8047 | step time 147.91ms\n",
            "step 4650 | loss 1.8196 | step time 148.07ms\n",
            "step 4675 | loss 1.8528 | step time 148.05ms\n",
            "step 4700 | loss 1.8508 | step time 148.14ms\n",
            "step 4725 | loss 1.8319 | step time 147.98ms\n",
            "step 4750 | loss 1.7869 | step time 148.12ms\n",
            "step 4775 | loss 1.8344 | step time 148.13ms\n",
            "step 4800 | loss 1.8448 | step time 148.06ms\n",
            "step 4800 train loss: 1.8305 test loss: 1.8584\n",
            "Test loss 1.8584 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4825 | loss 1.8051 | step time 147.92ms\n",
            "step 4850 | loss 1.7955 | step time 147.90ms\n",
            "step 4875 | loss 1.9152 | step time 148.03ms\n",
            "step 4900 | loss 1.8025 | step time 148.06ms\n",
            "step 4925 | loss 1.7991 | step time 148.15ms\n",
            "step 4950 | loss 1.7604 | step time 147.92ms\n",
            "step 4975 | loss 1.8337 | step time 148.33ms\n",
            "step 5000 | loss 1.8314 | step time 148.28ms\n",
            "step 5000 train loss: 1.8194 test loss: 1.8592\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 5025 | loss 1.8066 | step time 148.01ms\n",
            "step 5050 | loss 1.8323 | step time 147.86ms\n",
            "step 5075 | loss 1.7712 | step time 148.17ms\n",
            "step 5100 | loss 1.8178 | step time 148.19ms\n",
            "step 5125 | loss 1.7796 | step time 148.10ms\n",
            "step 5150 | loss 1.7886 | step time 148.08ms\n",
            "step 5175 | loss 1.8216 | step time 148.19ms\n",
            "step 5200 | loss 1.7853 | step time 148.07ms\n",
            "step 5200 train loss: 1.8177 test loss: 1.8498\n",
            "Test loss 1.8498 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 5225 | loss 1.8102 | step time 147.88ms\n",
            "step 5250 | loss 1.8258 | step time 147.90ms\n",
            "step 5275 | loss 1.8455 | step time 147.94ms\n",
            "step 5300 | loss 1.7530 | step time 148.12ms\n",
            "step 5325 | loss 1.8383 | step time 148.02ms\n",
            "step 5350 | loss 1.8446 | step time 147.95ms\n",
            "step 5375 | loss 1.7098 | step time 148.34ms\n",
            "step 5400 | loss 1.8269 | step time 148.15ms\n",
            "step 5400 train loss: 1.8079 test loss: 1.8424\n",
            "Test loss 1.8424 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 5425 | loss 1.7848 | step time 147.85ms\n",
            "step 5450 | loss 1.8659 | step time 147.91ms\n",
            "step 5475 | loss 1.7946 | step time 147.94ms\n",
            "step 5500 | loss 1.8538 | step time 148.18ms\n",
            "step 5525 | loss 1.8186 | step time 147.94ms\n",
            "step 5550 | loss 1.8463 | step time 148.12ms\n",
            "step 5575 | loss 1.8246 | step time 148.12ms\n",
            "step 5600 | loss 1.7967 | step time 149.47ms\n",
            "step 5600 train loss: 1.8220 test loss: 1.8438\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 5625 | loss 1.8326 | step time 147.94ms\n",
            "step 5650 | loss 1.8607 | step time 147.93ms\n",
            "step 5675 | loss 1.7861 | step time 148.18ms\n",
            "step 5700 | loss 1.8608 | step time 147.97ms\n",
            "step 5725 | loss 1.7659 | step time 147.90ms\n",
            "step 5750 | loss 1.7878 | step time 148.19ms\n",
            "step 5775 | loss 1.8195 | step time 148.33ms\n",
            "step 5800 | loss 1.8743 | step time 148.30ms\n",
            "step 5800 train loss: 1.8157 test loss: 1.8497\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 5825 | loss 1.8168 | step time 147.83ms\n",
            "step 5850 | loss 1.8404 | step time 148.25ms\n",
            "step 5875 | loss 1.7893 | step time 148.64ms\n",
            "step 5900 | loss 1.8554 | step time 148.13ms\n",
            "step 5925 | loss 1.8289 | step time 148.14ms\n",
            "step 5950 | loss 1.7556 | step time 148.01ms\n",
            "step 5975 | loss 1.8121 | step time 148.00ms\n",
            "step 6000 | loss 1.8219 | step time 148.18ms\n",
            "step 6000 train loss: 1.8093 test loss: 1.8469\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6025 | loss 1.8460 | step time 147.95ms\n",
            "step 6050 | loss 1.8110 | step time 147.89ms\n",
            "step 6075 | loss 1.8764 | step time 148.30ms\n",
            "step 6100 | loss 1.7797 | step time 148.06ms\n",
            "step 6125 | loss 1.8367 | step time 147.96ms\n",
            "step 6150 | loss 1.8025 | step time 148.14ms\n",
            "step 6175 | loss 1.8244 | step time 148.05ms\n",
            "step 6200 | loss 1.8524 | step time 148.01ms\n",
            "step 6200 train loss: 1.8125 test loss: 1.8413\n",
            "Test loss 1.8413 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6225 | loss 1.8234 | step time 147.91ms\n",
            "step 6250 | loss 1.8374 | step time 147.87ms\n",
            "step 6275 | loss 1.8062 | step time 148.14ms\n",
            "step 6300 | loss 1.7624 | step time 147.95ms\n",
            "step 6325 | loss 1.7911 | step time 147.98ms\n",
            "step 6350 | loss 1.8212 | step time 147.98ms\n",
            "step 6375 | loss 1.7428 | step time 148.06ms\n",
            "step 6400 | loss 1.7448 | step time 148.01ms\n",
            "step 6400 train loss: 1.8088 test loss: 1.8383\n",
            "Test loss 1.8383 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6425 | loss 1.7611 | step time 147.97ms\n",
            "step 6450 | loss 1.8784 | step time 147.69ms\n",
            "step 6475 | loss 1.7938 | step time 148.20ms\n",
            "step 6500 | loss 1.8182 | step time 148.00ms\n",
            "step 6525 | loss 1.8232 | step time 148.22ms\n",
            "step 6550 | loss 1.8465 | step time 148.28ms\n",
            "step 6575 | loss 1.8261 | step time 148.00ms\n",
            "step 6600 | loss 1.7815 | step time 148.07ms\n",
            "step 6600 train loss: 1.8123 test loss: 1.8326\n",
            "Test loss 1.8326 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6625 | loss 1.8099 | step time 148.01ms\n",
            "step 6650 | loss 1.8510 | step time 148.02ms\n",
            "step 6675 | loss 1.7892 | step time 148.46ms\n",
            "step 6700 | loss 1.7820 | step time 148.31ms\n",
            "step 6725 | loss 1.7661 | step time 147.98ms\n",
            "step 6750 | loss 1.8424 | step time 148.44ms\n",
            "step 6775 | loss 1.8517 | step time 148.19ms\n",
            "step 6800 | loss 1.8129 | step time 148.05ms\n",
            "step 6800 train loss: 1.7841 test loss: 1.8317\n",
            "Test loss 1.8317 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6825 | loss 1.8058 | step time 147.83ms\n",
            "step 6850 | loss 1.7977 | step time 147.80ms\n",
            "step 6875 | loss 1.8014 | step time 148.32ms\n",
            "step 6900 | loss 1.8065 | step time 147.88ms\n",
            "step 6925 | loss 1.7870 | step time 147.97ms\n",
            "step 6950 | loss 1.7959 | step time 148.26ms\n",
            "step 6975 | loss 1.7679 | step time 148.08ms\n",
            "step 7000 | loss 1.7537 | step time 148.01ms\n",
            "step 7000 train loss: 1.7921 test loss: 1.8266\n",
            "Test loss 1.8266 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 7025 | loss 1.7716 | step time 147.81ms\n",
            "step 7050 | loss 1.8432 | step time 148.15ms\n",
            "step 7075 | loss 1.8098 | step time 148.30ms\n",
            "step 7100 | loss 1.8407 | step time 148.06ms\n",
            "step 7125 | loss 1.7766 | step time 147.95ms\n",
            "step 7150 | loss 1.7599 | step time 148.63ms\n",
            "step 7175 | loss 1.8296 | step time 148.17ms\n",
            "step 7200 | loss 1.8305 | step time 148.04ms\n",
            "step 7200 train loss: 1.8008 test loss: 1.8339\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 7225 | loss 1.8217 | step time 148.16ms\n",
            "step 7250 | loss 1.8258 | step time 148.41ms\n",
            "step 7275 | loss 1.8280 | step time 148.29ms\n",
            "step 7300 | loss 1.7274 | step time 148.19ms\n",
            "step 7325 | loss 1.7976 | step time 147.89ms\n",
            "step 7350 | loss 1.7747 | step time 148.10ms\n",
            "step 7375 | loss 1.8051 | step time 148.14ms\n",
            "step 7400 | loss 1.8339 | step time 147.99ms\n",
            "step 7400 train loss: 1.7972 test loss: 1.8201\n",
            "Test loss 1.8201 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 7425 | loss 1.8079 | step time 147.93ms\n",
            "step 7450 | loss 1.8088 | step time 147.99ms\n",
            "step 7475 | loss 1.7735 | step time 148.02ms\n",
            "step 7500 | loss 1.7822 | step time 148.02ms\n",
            "step 7525 | loss 1.7331 | step time 148.29ms\n",
            "step 7550 | loss 1.8426 | step time 148.20ms\n",
            "step 7575 | loss 1.7047 | step time 148.13ms\n",
            "step 7600 | loss 1.7556 | step time 148.02ms\n",
            "step 7600 train loss: 1.7725 test loss: 1.8239\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 7625 | loss 1.7713 | step time 147.93ms\n",
            "step 7650 | loss 1.8051 | step time 148.16ms\n",
            "step 7675 | loss 1.7688 | step time 148.27ms\n",
            "step 7700 | loss 1.8051 | step time 148.04ms\n",
            "step 7725 | loss 1.8007 | step time 147.91ms\n",
            "step 7750 | loss 1.8420 | step time 148.20ms\n",
            "step 7775 | loss 1.7807 | step time 148.18ms\n",
            "step 7800 | loss 1.7293 | step time 148.36ms\n",
            "step 7800 train loss: 1.7832 test loss: 1.8261\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 7825 | loss 1.7999 | step time 148.05ms\n",
            "step 7850 | loss 1.7882 | step time 148.13ms\n",
            "step 7875 | loss 1.8216 | step time 147.90ms\n",
            "step 7900 | loss 1.7736 | step time 147.84ms\n",
            "step 7925 | loss 1.8543 | step time 148.40ms\n",
            "step 7950 | loss 1.7956 | step time 148.02ms\n",
            "step 7975 | loss 1.8317 | step time 148.12ms\n",
            "step 8000 | loss 1.7828 | step time 148.12ms\n",
            "step 8000 train loss: 1.7777 test loss: 1.8228\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8025 | loss 1.7473 | step time 148.58ms\n",
            "step 8050 | loss 1.8316 | step time 148.17ms\n",
            "step 8075 | loss 1.8468 | step time 147.95ms\n",
            "step 8100 | loss 1.7968 | step time 148.13ms\n",
            "step 8125 | loss 1.7515 | step time 148.13ms\n",
            "step 8150 | loss 1.7927 | step time 148.00ms\n",
            "step 8175 | loss 1.7830 | step time 148.01ms\n",
            "step 8200 | loss 1.7453 | step time 148.23ms\n",
            "step 8200 train loss: 1.7866 test loss: 1.8208\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8225 | loss 1.7861 | step time 147.82ms\n",
            "step 8250 | loss 1.7951 | step time 148.35ms\n",
            "step 8275 | loss 1.7042 | step time 148.02ms\n",
            "step 8300 | loss 1.8131 | step time 148.00ms\n",
            "step 8325 | loss 1.8066 | step time 148.53ms\n",
            "step 8350 | loss 1.8571 | step time 147.98ms\n",
            "step 8375 | loss 1.8146 | step time 148.20ms\n",
            "step 8400 | loss 1.7730 | step time 148.04ms\n",
            "step 8400 train loss: 1.7984 test loss: 1.8244\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8425 | loss 1.7278 | step time 148.02ms\n",
            "step 8450 | loss 1.6839 | step time 148.04ms\n",
            "step 8475 | loss 1.7759 | step time 147.95ms\n",
            "step 8500 | loss 1.8383 | step time 147.76ms\n",
            "step 8525 | loss 1.7548 | step time 148.18ms\n",
            "step 8550 | loss 1.7650 | step time 147.96ms\n",
            "step 8575 | loss 1.7841 | step time 147.94ms\n",
            "step 8600 | loss 1.7777 | step time 148.12ms\n",
            "step 8600 train loss: 1.7775 test loss: 1.8220\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8625 | loss 1.7507 | step time 147.88ms\n",
            "step 8650 | loss 1.7751 | step time 147.81ms\n",
            "step 8675 | loss 1.8389 | step time 147.83ms\n",
            "step 8700 | loss 1.8319 | step time 148.09ms\n",
            "step 8725 | loss 1.7453 | step time 148.42ms\n",
            "step 8750 | loss 1.8454 | step time 148.02ms\n",
            "step 8775 | loss 1.8093 | step time 147.84ms\n",
            "step 8800 | loss 1.7415 | step time 148.12ms\n",
            "step 8800 train loss: 1.7811 test loss: 1.8188\n",
            "Test loss 1.8188 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8825 | loss 1.7891 | step time 147.86ms\n",
            "step 8850 | loss 1.8825 | step time 147.93ms\n",
            "step 8875 | loss 1.7921 | step time 147.84ms\n",
            "step 8900 | loss 1.7392 | step time 148.13ms\n",
            "step 8925 | loss 1.8072 | step time 148.35ms\n",
            "step 8950 | loss 1.7955 | step time 148.20ms\n",
            "step 8975 | loss 1.7375 | step time 147.90ms\n",
            "step 9000 | loss 1.8119 | step time 148.47ms\n",
            "step 9000 train loss: 1.7872 test loss: 1.8237\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 9025 | loss 1.8386 | step time 147.95ms\n",
            "step 9050 | loss 1.7305 | step time 148.05ms\n",
            "step 9075 | loss 1.8819 | step time 147.96ms\n",
            "step 9100 | loss 1.7687 | step time 147.85ms\n",
            "step 9125 | loss 1.8087 | step time 147.84ms\n",
            "step 9150 | loss 1.7819 | step time 147.93ms\n",
            "step 9175 | loss 1.8038 | step time 147.98ms\n",
            "step 9200 | loss 1.7162 | step time 148.30ms\n",
            "step 9200 train loss: 1.7722 test loss: 1.8219\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 9225 | loss 1.7829 | step time 148.24ms\n",
            "step 9250 | loss 1.8603 | step time 147.86ms\n",
            "step 9275 | loss 1.8094 | step time 147.97ms\n",
            "step 9300 | loss 1.7533 | step time 148.17ms\n",
            "step 9325 | loss 1.7437 | step time 148.16ms\n",
            "step 9350 | loss 1.7875 | step time 147.89ms\n",
            "step 9375 | loss 1.7764 | step time 147.92ms\n",
            "step 9400 | loss 1.7907 | step time 148.26ms\n",
            "step 9400 train loss: 1.7676 test loss: 1.8216\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 9425 | loss 1.7835 | step time 148.21ms\n",
            "step 9450 | loss 1.8023 | step time 148.02ms\n",
            "step 9475 | loss 1.8129 | step time 148.22ms\n",
            "step 9500 | loss 1.7143 | step time 147.87ms\n",
            "step 9525 | loss 1.8571 | step time 148.26ms\n",
            "step 9550 | loss 1.7667 | step time 147.90ms\n",
            "step 9575 | loss 1.7775 | step time 148.00ms\n",
            "step 9600 | loss 1.8430 | step time 148.45ms\n",
            "step 9600 train loss: 1.7935 test loss: 1.8245\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 9625 | loss 1.7030 | step time 148.13ms\n",
            "step 9650 | loss 1.7762 | step time 148.01ms\n",
            "step 9675 | loss 1.7390 | step time 147.91ms\n",
            "step 9700 | loss 1.7729 | step time 147.95ms\n",
            "step 9725 | loss 1.8392 | step time 147.99ms\n",
            "step 9750 | loss 1.7880 | step time 148.01ms\n",
            "step 9775 | loss 1.7472 | step time 148.01ms\n",
            "step 9800 | loss 1.7880 | step time 148.26ms\n",
            "step 9800 train loss: 1.7925 test loss: 1.8153\n",
            "Test loss 1.8153 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 9825 | loss 1.8112 | step time 147.98ms\n",
            "step 9850 | loss 1.8064 | step time 148.03ms\n",
            "step 9875 | loss 1.7681 | step time 148.21ms\n",
            "step 9900 | loss 1.7496 | step time 148.11ms\n",
            "step 9925 | loss 1.8037 | step time 147.82ms\n",
            "step 9950 | loss 1.8072 | step time 148.02ms\n",
            "step 9975 | loss 1.7817 | step time 148.21ms\n",
            "step 10000 | loss 1.7930 | step time 147.92ms\n",
            "step 10000 train loss: 1.7603 test loss: 1.8230\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10025 | loss 1.7967 | step time 147.88ms\n",
            "step 10050 | loss 1.7673 | step time 147.97ms\n",
            "step 10075 | loss 1.7943 | step time 147.83ms\n",
            "step 10100 | loss 1.7945 | step time 148.16ms\n",
            "step 10125 | loss 1.7589 | step time 147.93ms\n",
            "step 10150 | loss 1.7356 | step time 148.20ms\n",
            "step 10175 | loss 1.7284 | step time 147.97ms\n",
            "step 10200 | loss 1.7693 | step time 148.03ms\n",
            "step 10200 train loss: 1.7627 test loss: 1.8198\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10225 | loss 1.7740 | step time 147.83ms\n",
            "step 10250 | loss 1.7892 | step time 147.91ms\n",
            "step 10275 | loss 1.7338 | step time 147.90ms\n",
            "step 10300 | loss 1.7536 | step time 167.95ms\n",
            "step 10325 | loss 1.7602 | step time 148.10ms\n",
            "step 10350 | loss 1.7482 | step time 148.10ms\n",
            "step 10375 | loss 1.7119 | step time 148.49ms\n",
            "step 10400 | loss 1.7446 | step time 148.06ms\n",
            "step 10400 train loss: 1.7733 test loss: 1.8098\n",
            "Test loss 1.8098 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10425 | loss 1.8054 | step time 148.12ms\n",
            "step 10450 | loss 1.7935 | step time 147.82ms\n",
            "step 10475 | loss 1.7500 | step time 147.92ms\n",
            "step 10500 | loss 1.8017 | step time 147.91ms\n",
            "step 10525 | loss 1.7133 | step time 148.28ms\n",
            "step 10550 | loss 1.6911 | step time 147.96ms\n",
            "step 10575 | loss 1.7841 | step time 148.02ms\n",
            "step 10600 | loss 1.7776 | step time 148.00ms\n",
            "step 10600 train loss: 1.7610 test loss: 1.8078\n",
            "Test loss 1.8078 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10625 | loss 1.7774 | step time 148.12ms\n",
            "step 10650 | loss 1.8396 | step time 148.03ms\n",
            "step 10675 | loss 1.8452 | step time 147.80ms\n",
            "step 10700 | loss 1.8026 | step time 148.06ms\n",
            "step 10725 | loss 1.7473 | step time 148.03ms\n",
            "step 10750 | loss 1.7134 | step time 148.07ms\n",
            "step 10775 | loss 1.7330 | step time 148.56ms\n",
            "step 10800 | loss 1.7660 | step time 148.12ms\n",
            "step 10800 train loss: 1.7597 test loss: 1.8126\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10825 | loss 1.7534 | step time 147.99ms\n",
            "step 10850 | loss 1.7175 | step time 147.87ms\n",
            "step 10875 | loss 1.8016 | step time 147.85ms\n",
            "step 10900 | loss 1.6931 | step time 147.93ms\n",
            "step 10925 | loss 1.7768 | step time 148.14ms\n",
            "step 10950 | loss 1.7747 | step time 147.99ms\n",
            "step 10975 | loss 1.8256 | step time 148.49ms\n",
            "step 11000 | loss 1.8553 | step time 147.95ms\n",
            "step 11000 train loss: 1.7748 test loss: 1.8130\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 11025 | loss 1.7409 | step time 148.07ms\n",
            "step 11050 | loss 1.6976 | step time 147.94ms\n",
            "step 11075 | loss 1.7771 | step time 148.16ms\n",
            "step 11100 | loss 1.8062 | step time 148.02ms\n",
            "step 11125 | loss 1.7697 | step time 148.05ms\n",
            "step 11150 | loss 1.7267 | step time 148.04ms\n",
            "step 11175 | loss 1.7490 | step time 148.16ms\n",
            "step 11200 | loss 1.7898 | step time 148.18ms\n",
            "step 11200 train loss: 1.7698 test loss: 1.8236\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 11225 | loss 1.7947 | step time 148.28ms\n",
            "step 11250 | loss 1.6972 | step time 148.24ms\n",
            "step 11275 | loss 1.8070 | step time 148.61ms\n",
            "step 11300 | loss 1.8543 | step time 148.65ms\n",
            "step 11325 | loss 1.7269 | step time 148.32ms\n",
            "step 11350 | loss 1.7842 | step time 148.48ms\n",
            "step 11375 | loss 1.7015 | step time 148.39ms\n",
            "step 11400 | loss 1.7519 | step time 148.50ms\n",
            "step 11400 train loss: 1.7692 test loss: 1.8158\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 11425 | loss 1.7799 | step time 148.41ms\n",
            "step 11450 | loss 1.7737 | step time 148.37ms\n",
            "step 11475 | loss 1.8298 | step time 148.48ms\n",
            "step 11500 | loss 1.7558 | step time 147.95ms\n",
            "step 11525 | loss 1.7518 | step time 148.21ms\n",
            "step 11550 | loss 1.7698 | step time 148.30ms\n",
            "step 11575 | loss 1.7559 | step time 148.20ms\n",
            "step 11600 | loss 1.8172 | step time 148.06ms\n",
            "step 11600 train loss: 1.7587 test loss: 1.8125\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 11625 | loss 1.7730 | step time 147.94ms\n",
            "step 11650 | loss 1.7405 | step time 147.89ms\n",
            "step 11675 | loss 1.7697 | step time 148.07ms\n",
            "step 11700 | loss 1.7576 | step time 147.88ms\n",
            "step 11725 | loss 1.7421 | step time 148.40ms\n",
            "step 11750 | loss 1.7521 | step time 148.53ms\n",
            "step 11775 | loss 1.7435 | step time 148.27ms\n",
            "step 11800 | loss 1.7997 | step time 148.23ms\n",
            "step 11800 train loss: 1.7620 test loss: 1.8103\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 11825 | loss 1.8112 | step time 148.08ms\n",
            "step 11850 | loss 1.8036 | step time 148.00ms\n",
            "step 11875 | loss 1.7957 | step time 148.42ms\n",
            "step 11900 | loss 1.7455 | step time 148.04ms\n",
            "step 11925 | loss 1.7706 | step time 148.42ms\n",
            "step 11950 | loss 1.7507 | step time 148.34ms\n",
            "step 11975 | loss 1.8050 | step time 148.19ms\n",
            "step 12000 | loss 1.7169 | step time 148.24ms\n",
            "step 12000 train loss: 1.7525 test loss: 1.8069\n",
            "Test loss 1.8069 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12025 | loss 1.7629 | step time 148.00ms\n",
            "step 12050 | loss 1.7470 | step time 147.95ms\n",
            "step 12075 | loss 1.7217 | step time 148.31ms\n",
            "step 12100 | loss 1.7890 | step time 147.96ms\n",
            "step 12125 | loss 1.7997 | step time 147.97ms\n",
            "step 12150 | loss 1.8051 | step time 148.34ms\n",
            "step 12175 | loss 1.7833 | step time 148.25ms\n",
            "step 12200 | loss 1.8076 | step time 148.05ms\n",
            "step 12200 train loss: 1.7549 test loss: 1.8059\n",
            "Test loss 1.8059 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12225 | loss 1.7674 | step time 148.19ms\n",
            "step 12250 | loss 1.7219 | step time 147.96ms\n",
            "step 12275 | loss 1.6983 | step time 148.07ms\n",
            "step 12300 | loss 1.7344 | step time 148.24ms\n",
            "step 12325 | loss 1.8518 | step time 148.21ms\n",
            "step 12350 | loss 1.7239 | step time 148.49ms\n",
            "step 12375 | loss 1.7336 | step time 147.99ms\n",
            "step 12400 | loss 1.7607 | step time 148.01ms\n",
            "step 12400 train loss: 1.7523 test loss: 1.8091\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12425 | loss 1.7636 | step time 147.99ms\n",
            "step 12450 | loss 1.7592 | step time 148.12ms\n",
            "step 12475 | loss 1.8522 | step time 147.98ms\n",
            "step 12500 | loss 1.7655 | step time 147.91ms\n",
            "step 12525 | loss 1.8115 | step time 148.04ms\n",
            "step 12550 | loss 1.7696 | step time 148.62ms\n",
            "step 12575 | loss 1.7495 | step time 148.10ms\n",
            "step 12600 | loss 1.7197 | step time 148.12ms\n",
            "step 12600 train loss: 1.7516 test loss: 1.8030\n",
            "Test loss 1.8030 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12625 | loss 1.8032 | step time 148.00ms\n",
            "step 12650 | loss 1.7119 | step time 148.03ms\n",
            "step 12675 | loss 1.7990 | step time 148.16ms\n",
            "step 12700 | loss 1.7882 | step time 147.79ms\n",
            "step 12725 | loss 1.7789 | step time 148.00ms\n",
            "step 12750 | loss 1.7280 | step time 148.49ms\n",
            "step 12775 | loss 1.7953 | step time 148.14ms\n",
            "step 12800 | loss 1.7366 | step time 148.27ms\n",
            "step 12800 train loss: 1.7376 test loss: 1.8136\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12825 | loss 1.7147 | step time 148.11ms\n",
            "step 12850 | loss 1.7592 | step time 148.42ms\n",
            "step 12875 | loss 1.7550 | step time 148.11ms\n",
            "step 12900 | loss 1.8939 | step time 148.12ms\n",
            "step 12925 | loss 1.7136 | step time 148.29ms\n",
            "step 12950 | loss 1.7680 | step time 148.63ms\n",
            "step 12975 | loss 1.6850 | step time 148.36ms\n",
            "step 13000 | loss 1.7462 | step time 148.24ms\n",
            "step 13000 train loss: 1.7541 test loss: 1.8116\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 13025 | loss 1.7700 | step time 147.96ms\n",
            "step 13050 | loss 1.7501 | step time 148.38ms\n",
            "step 13075 | loss 1.6731 | step time 147.98ms\n",
            "step 13100 | loss 1.7621 | step time 147.94ms\n",
            "step 13125 | loss 1.7513 | step time 148.26ms\n",
            "step 13150 | loss 1.6990 | step time 148.07ms\n",
            "step 13175 | loss 1.7473 | step time 148.13ms\n",
            "step 13200 | loss 1.6754 | step time 148.04ms\n",
            "step 13200 train loss: 1.7532 test loss: 1.8055\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 13225 | loss 1.7145 | step time 148.10ms\n",
            "step 13250 | loss 1.7638 | step time 148.42ms\n",
            "step 13275 | loss 1.7916 | step time 147.92ms\n",
            "step 13300 | loss 1.7021 | step time 147.92ms\n",
            "step 13325 | loss 1.7924 | step time 148.09ms\n",
            "step 13350 | loss 1.7489 | step time 147.90ms\n",
            "step 13375 | loss 1.7979 | step time 148.12ms\n",
            "step 13400 | loss 1.8199 | step time 148.01ms\n",
            "step 13400 train loss: 1.7551 test loss: 1.8184\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 13425 | loss 1.7715 | step time 147.91ms\n",
            "step 13450 | loss 1.7598 | step time 148.07ms\n",
            "step 13475 | loss 1.7756 | step time 147.98ms\n",
            "step 13500 | loss 1.7347 | step time 148.04ms\n",
            "step 13525 | loss 1.6753 | step time 148.02ms\n",
            "step 13550 | loss 1.7527 | step time 148.04ms\n",
            "step 13575 | loss 1.7417 | step time 148.17ms\n",
            "step 13600 | loss 1.7441 | step time 148.44ms\n",
            "step 13600 train loss: 1.7527 test loss: 1.8136\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 13625 | loss 1.7553 | step time 148.02ms\n",
            "step 13650 | loss 1.7839 | step time 148.11ms\n",
            "step 13675 | loss 1.8197 | step time 147.83ms\n",
            "step 13700 | loss 1.7496 | step time 148.02ms\n",
            "step 13725 | loss 1.7657 | step time 148.44ms\n",
            "step 13750 | loss 1.8072 | step time 147.85ms\n",
            "step 13775 | loss 1.7728 | step time 148.07ms\n",
            "step 13800 | loss 1.7304 | step time 148.24ms\n",
            "step 13800 train loss: 1.7490 test loss: 1.8143\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 13825 | loss 1.7892 | step time 147.79ms\n",
            "step 13850 | loss 1.8233 | step time 148.07ms\n",
            "step 13875 | loss 1.7685 | step time 148.27ms\n",
            "step 13900 | loss 1.7124 | step time 147.99ms\n",
            "step 13925 | loss 1.7182 | step time 148.41ms\n",
            "step 13950 | loss 1.8437 | step time 147.94ms\n",
            "step 13975 | loss 1.7197 | step time 148.06ms\n",
            "step 14000 | loss 1.6723 | step time 148.31ms\n",
            "step 14000 train loss: 1.7334 test loss: 1.8078\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14025 | loss 1.7016 | step time 148.32ms\n",
            "step 14050 | loss 1.7552 | step time 147.80ms\n",
            "step 14075 | loss 1.7545 | step time 148.03ms\n",
            "step 14100 | loss 1.7895 | step time 147.89ms\n",
            "step 14125 | loss 1.7874 | step time 147.91ms\n",
            "step 14150 | loss 1.7347 | step time 147.92ms\n",
            "step 14175 | loss 1.8034 | step time 147.85ms\n",
            "step 14200 | loss 1.7398 | step time 148.20ms\n",
            "step 14200 train loss: 1.7412 test loss: 1.8085\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14225 | loss 1.7488 | step time 148.08ms\n",
            "step 14250 | loss 1.7539 | step time 147.79ms\n",
            "step 14275 | loss 1.7842 | step time 147.88ms\n",
            "step 14300 | loss 1.7588 | step time 147.81ms\n",
            "step 14325 | loss 1.7475 | step time 147.87ms\n",
            "step 14350 | loss 1.7740 | step time 147.74ms\n",
            "step 14375 | loss 1.6895 | step time 148.11ms\n",
            "step 14400 | loss 1.7242 | step time 148.42ms\n",
            "step 14400 train loss: 1.7440 test loss: 1.7996\n",
            "Test loss 1.7996 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14425 | loss 1.7456 | step time 148.09ms\n",
            "step 14450 | loss 1.7614 | step time 148.01ms\n",
            "step 14475 | loss 1.7224 | step time 148.05ms\n",
            "step 14500 | loss 1.7517 | step time 147.83ms\n",
            "step 14525 | loss 1.7928 | step time 148.09ms\n",
            "step 14550 | loss 1.6844 | step time 147.80ms\n",
            "step 14575 | loss 1.7761 | step time 148.02ms\n",
            "step 14600 | loss 1.7809 | step time 148.43ms\n",
            "step 14600 train loss: 1.7716 test loss: 1.8042\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14625 | loss 1.7336 | step time 148.66ms\n",
            "step 14650 | loss 1.6929 | step time 148.08ms\n",
            "step 14675 | loss 1.7921 | step time 148.04ms\n",
            "step 14700 | loss 1.7856 | step time 147.93ms\n",
            "step 14725 | loss 1.6935 | step time 147.80ms\n",
            "step 14750 | loss 1.6584 | step time 148.06ms\n",
            "step 14775 | loss 1.7230 | step time 148.07ms\n",
            "step 14800 | loss 1.7784 | step time 148.21ms\n",
            "step 14800 train loss: 1.7490 test loss: 1.8083\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14825 | loss 1.7028 | step time 147.97ms\n",
            "step 14850 | loss 1.7157 | step time 147.86ms\n",
            "step 14875 | loss 1.7127 | step time 147.90ms\n",
            "step 14900 | loss 1.7678 | step time 148.26ms\n",
            "step 14925 | loss 1.7179 | step time 147.93ms\n",
            "step 14950 | loss 1.7355 | step time 148.07ms\n",
            "step 14975 | loss 1.7330 | step time 148.01ms\n",
            "step 15000 | loss 1.7208 | step time 148.24ms\n",
            "step 15000 train loss: 1.7297 test loss: 1.8093\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 15025 | loss 1.7810 | step time 148.29ms\n",
            "step 15050 | loss 1.7967 | step time 147.95ms\n",
            "step 15075 | loss 1.7309 | step time 148.10ms\n",
            "step 15100 | loss 1.7721 | step time 148.15ms\n",
            "step 15125 | loss 1.6794 | step time 147.86ms\n",
            "step 15150 | loss 1.7443 | step time 147.91ms\n",
            "step 15175 | loss 1.8201 | step time 148.04ms\n",
            "step 15200 | loss 1.7717 | step time 148.10ms\n",
            "step 15200 train loss: 1.7386 test loss: 1.8044\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 15225 | loss 1.7152 | step time 148.50ms\n",
            "step 15250 | loss 1.6852 | step time 147.97ms\n",
            "step 15275 | loss 1.7267 | step time 147.85ms\n",
            "step 15300 | loss 1.7654 | step time 148.05ms\n",
            "step 15325 | loss 1.6396 | step time 148.19ms\n",
            "step 15350 | loss 1.7430 | step time 148.53ms\n",
            "step 15375 | loss 1.6766 | step time 148.32ms\n",
            "step 15400 | loss 1.7666 | step time 147.92ms\n",
            "step 15400 train loss: 1.7484 test loss: 1.8111\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 15425 | loss 1.6999 | step time 148.22ms\n",
            "step 15450 | loss 1.7324 | step time 148.14ms\n",
            "step 15475 | loss 1.7754 | step time 147.81ms\n",
            "step 15500 | loss 1.6700 | step time 148.31ms\n",
            "step 15525 | loss 1.8045 | step time 147.87ms\n",
            "step 15550 | loss 1.7139 | step time 148.02ms\n",
            "step 15575 | loss 1.7236 | step time 148.08ms\n",
            "step 15600 | loss 1.8049 | step time 147.96ms\n",
            "step 15600 train loss: 1.7368 test loss: 1.8012\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 15625 | loss 1.7346 | step time 148.12ms\n",
            "step 15650 | loss 1.6919 | step time 147.88ms\n",
            "step 15675 | loss 1.7285 | step time 147.92ms\n",
            "step 15700 | loss 1.6502 | step time 148.39ms\n",
            "step 15725 | loss 1.7990 | step time 147.96ms\n",
            "step 15750 | loss 1.7270 | step time 148.17ms\n",
            "step 15775 | loss 1.7515 | step time 148.17ms\n",
            "step 15800 | loss 1.7758 | step time 148.01ms\n",
            "step 15800 train loss: 1.7302 test loss: 1.8135\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 15825 | loss 1.7560 | step time 148.10ms\n",
            "step 15850 | loss 1.7354 | step time 148.27ms\n",
            "step 15875 | loss 1.7179 | step time 147.92ms\n",
            "step 15900 | loss 1.7737 | step time 148.18ms\n",
            "step 15925 | loss 1.7516 | step time 148.05ms\n",
            "step 15950 | loss 1.7251 | step time 147.78ms\n",
            "step 15975 | loss 1.7397 | step time 148.44ms\n",
            "step 16000 | loss 1.7886 | step time 147.80ms\n",
            "step 16000 train loss: 1.7173 test loss: 1.8051\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16025 | loss 1.7151 | step time 148.12ms\n",
            "step 16050 | loss 1.7389 | step time 147.88ms\n",
            "step 16075 | loss 1.8051 | step time 147.83ms\n",
            "step 16100 | loss 1.7654 | step time 148.22ms\n",
            "step 16125 | loss 1.6980 | step time 147.94ms\n",
            "step 16150 | loss 1.7246 | step time 148.02ms\n",
            "step 16175 | loss 1.7597 | step time 148.10ms\n",
            "step 16200 | loss 1.7012 | step time 147.88ms\n",
            "step 16200 train loss: 1.7280 test loss: 1.8126\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16225 | loss 1.7059 | step time 148.11ms\n",
            "step 16250 | loss 1.7545 | step time 148.09ms\n",
            "step 16275 | loss 1.7143 | step time 148.08ms\n",
            "step 16300 | loss 1.6756 | step time 148.57ms\n",
            "step 16325 | loss 1.7103 | step time 147.94ms\n",
            "step 16350 | loss 1.7523 | step time 148.09ms\n",
            "step 16375 | loss 1.7243 | step time 148.26ms\n",
            "step 16400 | loss 1.7367 | step time 148.21ms\n",
            "step 16400 train loss: 1.7334 test loss: 1.8060\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16425 | loss 1.7816 | step time 147.92ms\n",
            "step 16450 | loss 1.7725 | step time 147.99ms\n",
            "step 16475 | loss 1.7811 | step time 147.85ms\n",
            "step 16500 | loss 1.6901 | step time 148.49ms\n",
            "step 16525 | loss 1.6962 | step time 148.17ms\n",
            "step 16550 | loss 1.7251 | step time 148.03ms\n",
            "step 16575 | loss 1.6959 | step time 148.32ms\n",
            "step 16600 | loss 1.7433 | step time 148.07ms\n",
            "step 16600 train loss: 1.7409 test loss: 1.8089\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16625 | loss 1.7073 | step time 147.91ms\n",
            "step 16650 | loss 1.6897 | step time 148.02ms\n",
            "step 16675 | loss 1.7428 | step time 147.95ms\n",
            "step 16700 | loss 1.7138 | step time 148.13ms\n",
            "step 16725 | loss 1.7225 | step time 148.20ms\n",
            "step 16750 | loss 1.7099 | step time 148.32ms\n",
            "step 16775 | loss 1.7166 | step time 147.95ms\n",
            "step 16800 | loss 1.7286 | step time 148.06ms\n",
            "step 16800 train loss: 1.7443 test loss: 1.8035\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16825 | loss 1.7328 | step time 147.93ms\n",
            "step 16850 | loss 1.7183 | step time 147.96ms\n",
            "step 16875 | loss 1.7594 | step time 148.24ms\n",
            "step 16900 | loss 1.7449 | step time 148.02ms\n",
            "step 16925 | loss 1.7584 | step time 147.93ms\n",
            "step 16950 | loss 1.7600 | step time 148.04ms\n",
            "step 16975 | loss 1.6599 | step time 148.38ms\n",
            "step 17000 | loss 1.7825 | step time 148.21ms\n",
            "step 17000 train loss: 1.7340 test loss: 1.7972\n",
            "Test loss 1.7972 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 17025 | loss 1.7041 | step time 147.99ms\n",
            "step 17050 | loss 1.7410 | step time 148.03ms\n",
            "step 17075 | loss 1.7642 | step time 148.18ms\n",
            "step 17100 | loss 1.7198 | step time 148.38ms\n",
            "step 17125 | loss 1.7207 | step time 148.33ms\n",
            "step 17150 | loss 1.7918 | step time 148.27ms\n",
            "step 17175 | loss 1.8122 | step time 148.39ms\n",
            "step 17200 | loss 1.7886 | step time 148.09ms\n",
            "step 17200 train loss: 1.7489 test loss: 1.8086\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 17225 | loss 1.7302 | step time 147.89ms\n",
            "step 17250 | loss 1.7522 | step time 147.94ms\n",
            "step 17275 | loss 1.7372 | step time 148.08ms\n",
            "step 17300 | loss 1.7292 | step time 148.08ms\n",
            "step 17325 | loss 1.7573 | step time 148.03ms\n",
            "step 17350 | loss 1.7776 | step time 148.00ms\n",
            "step 17375 | loss 1.6878 | step time 148.06ms\n",
            "step 17400 | loss 1.7560 | step time 148.27ms\n",
            "step 17400 train loss: 1.7351 test loss: 1.8067\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 17425 | loss 1.7006 | step time 147.95ms\n",
            "step 17450 | loss 1.7964 | step time 148.00ms\n",
            "step 17475 | loss 1.6985 | step time 148.35ms\n",
            "step 17500 | loss 1.6849 | step time 148.03ms\n",
            "step 17525 | loss 1.6972 | step time 148.05ms\n",
            "step 17550 | loss 1.7485 | step time 148.03ms\n",
            "step 17575 | loss 1.6735 | step time 148.25ms\n",
            "step 17600 | loss 1.6849 | step time 148.11ms\n",
            "step 17600 train loss: 1.7487 test loss: 1.8033\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 17625 | loss 1.7080 | step time 147.97ms\n",
            "step 17650 | loss 1.7317 | step time 147.99ms\n",
            "step 17675 | loss 1.6873 | step time 148.21ms\n",
            "step 17700 | loss 1.7310 | step time 148.03ms\n",
            "step 17725 | loss 1.7283 | step time 147.93ms\n",
            "step 17750 | loss 1.7602 | step time 148.47ms\n",
            "step 17775 | loss 1.7103 | step time 148.12ms\n",
            "step 17800 | loss 1.7817 | step time 148.17ms\n",
            "step 17800 train loss: 1.7301 test loss: 1.8116\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 17825 | loss 1.7833 | step time 148.24ms\n",
            "step 17850 | loss 1.7300 | step time 147.85ms\n",
            "step 17875 | loss 1.7459 | step time 148.13ms\n",
            "step 17900 | loss 1.7828 | step time 147.94ms\n",
            "step 17925 | loss 1.7136 | step time 148.12ms\n",
            "step 17950 | loss 1.7482 | step time 148.48ms\n",
            "step 17975 | loss 1.7145 | step time 148.05ms\n",
            "step 18000 | loss 1.7623 | step time 148.21ms\n",
            "step 18000 train loss: 1.7376 test loss: 1.8047\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18025 | loss 1.7839 | step time 148.06ms\n",
            "step 18050 | loss 1.7161 | step time 147.89ms\n",
            "step 18075 | loss 1.7960 | step time 148.38ms\n",
            "step 18100 | loss 1.7451 | step time 147.91ms\n",
            "step 18125 | loss 1.7753 | step time 147.91ms\n",
            "step 18150 | loss 1.7227 | step time 148.22ms\n",
            "step 18175 | loss 1.7615 | step time 147.90ms\n",
            "step 18200 | loss 1.7206 | step time 148.01ms\n",
            "step 18200 train loss: 1.7416 test loss: 1.8091\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18225 | loss 1.7595 | step time 147.96ms\n",
            "step 18250 | loss 1.6908 | step time 148.01ms\n",
            "step 18275 | loss 1.7707 | step time 148.39ms\n",
            "step 18300 | loss 1.7285 | step time 148.00ms\n",
            "step 18325 | loss 1.7046 | step time 148.00ms\n",
            "step 18350 | loss 1.7186 | step time 148.25ms\n",
            "step 18375 | loss 1.7601 | step time 148.19ms\n",
            "step 18400 | loss 1.7467 | step time 148.13ms\n",
            "step 18400 train loss: 1.7500 test loss: 1.8099\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18425 | loss 1.6473 | step time 147.99ms\n",
            "step 18450 | loss 1.6700 | step time 148.16ms\n",
            "step 18475 | loss 1.7833 | step time 148.19ms\n",
            "step 18500 | loss 1.7078 | step time 147.93ms\n",
            "step 18525 | loss 1.7357 | step time 147.81ms\n",
            "step 18550 | loss 1.7494 | step time 148.00ms\n",
            "step 18575 | loss 1.7537 | step time 147.81ms\n",
            "step 18600 | loss 1.7386 | step time 147.99ms\n",
            "step 18600 train loss: 1.7353 test loss: 1.8135\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18625 | loss 1.7531 | step time 147.97ms\n",
            "step 18650 | loss 1.7478 | step time 147.87ms\n",
            "step 18675 | loss 1.6750 | step time 148.07ms\n",
            "step 18700 | loss 1.7233 | step time 147.95ms\n",
            "step 18725 | loss 1.7854 | step time 148.08ms\n",
            "step 18750 | loss 1.6961 | step time 148.21ms\n",
            "step 18775 | loss 1.8006 | step time 148.00ms\n",
            "step 18800 | loss 1.6753 | step time 148.02ms\n",
            "step 18800 train loss: 1.7173 test loss: 1.8047\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18825 | loss 1.7011 | step time 147.92ms\n",
            "step 18850 | loss 1.7559 | step time 147.91ms\n",
            "step 18875 | loss 1.6851 | step time 148.17ms\n",
            "step 18900 | loss 1.7159 | step time 147.93ms\n",
            "step 18925 | loss 1.7834 | step time 147.89ms\n",
            "step 18950 | loss 1.7777 | step time 148.56ms\n",
            "step 18975 | loss 1.7072 | step time 148.16ms\n",
            "step 19000 | loss 1.7889 | step time 148.01ms\n",
            "step 19000 train loss: 1.7427 test loss: 1.8144\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 19025 | loss 1.7943 | step time 147.86ms\n",
            "step 19050 | loss 1.7132 | step time 148.12ms\n",
            "step 19075 | loss 1.7329 | step time 147.83ms\n",
            "step 19100 | loss 1.7096 | step time 147.82ms\n",
            "step 19125 | loss 1.7330 | step time 147.95ms\n",
            "step 19150 | loss 1.7344 | step time 148.20ms\n",
            "step 19175 | loss 1.7182 | step time 147.90ms\n",
            "step 19200 | loss 1.7199 | step time 148.05ms\n",
            "step 19200 train loss: 1.7212 test loss: 1.8098\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 19225 | loss 1.7318 | step time 148.01ms\n",
            "step 19250 | loss 1.7417 | step time 147.95ms\n",
            "step 19275 | loss 1.7139 | step time 147.90ms\n",
            "step 19300 | loss 1.7486 | step time 147.85ms\n",
            "step 19325 | loss 1.7503 | step time 147.87ms\n",
            "step 19350 | loss 1.7068 | step time 148.38ms\n",
            "step 19375 | loss 1.7960 | step time 148.19ms\n",
            "step 19400 | loss 1.7339 | step time 148.02ms\n",
            "step 19400 train loss: 1.7218 test loss: 1.8091\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 19425 | loss 1.7459 | step time 148.02ms\n",
            "step 19450 | loss 1.7111 | step time 147.99ms\n",
            "step 19475 | loss 1.7037 | step time 148.01ms\n",
            "step 19500 | loss 1.8071 | step time 147.95ms\n",
            "step 19525 | loss 1.7093 | step time 147.94ms\n",
            "step 19550 | loss 1.7319 | step time 147.95ms\n",
            "step 19575 | loss 1.8252 | step time 147.83ms\n",
            "step 19600 | loss 1.6465 | step time 148.10ms\n",
            "step 19600 train loss: 1.7275 test loss: 1.8038\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 19625 | loss 1.6524 | step time 147.94ms\n",
            "step 19650 | loss 1.7013 | step time 148.11ms\n",
            "step 19675 | loss 1.6695 | step time 148.06ms\n",
            "step 19700 | loss 1.7190 | step time 148.11ms\n",
            "step 19725 | loss 1.7204 | step time 148.04ms\n",
            "step 19750 | loss 1.7360 | step time 148.01ms\n",
            "step 19775 | loss 1.7594 | step time 148.10ms\n",
            "step 19800 | loss 1.6918 | step time 148.27ms\n",
            "step 19800 train loss: 1.7176 test loss: 1.8117\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 19825 | loss 1.6656 | step time 148.33ms\n",
            "step 19850 | loss 1.7453 | step time 148.43ms\n",
            "step 19875 | loss 1.7529 | step time 148.26ms\n",
            "step 19900 | loss 1.7115 | step time 148.54ms\n",
            "step 19925 | loss 1.8120 | step time 148.23ms\n",
            "step 19950 | loss 1.6986 | step time 148.35ms\n",
            "step 19975 | loss 1.7717 | step time 147.95ms\n",
            "step 20000 | loss 1.7815 | step time 148.37ms\n",
            "step 20000 train loss: 1.7256 test loss: 1.8175\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20025 | loss 1.7703 | step time 147.93ms\n",
            "step 20050 | loss 1.7209 | step time 148.37ms\n",
            "step 20075 | loss 1.7485 | step time 148.24ms\n",
            "step 20100 | loss 1.7126 | step time 147.93ms\n",
            "step 20125 | loss 1.6818 | step time 148.12ms\n",
            "step 20150 | loss 1.7370 | step time 148.06ms\n",
            "step 20175 | loss 1.7219 | step time 148.44ms\n",
            "step 20200 | loss 1.7042 | step time 148.07ms\n",
            "step 20200 train loss: 1.7312 test loss: 1.8110\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20225 | loss 1.7819 | step time 148.10ms\n",
            "step 20250 | loss 1.7399 | step time 148.22ms\n",
            "step 20275 | loss 1.8112 | step time 148.06ms\n",
            "step 20300 | loss 1.6624 | step time 148.24ms\n",
            "step 20325 | loss 1.7377 | step time 148.17ms\n",
            "step 20350 | loss 1.8153 | step time 148.28ms\n",
            "step 20375 | loss 1.6405 | step time 148.05ms\n",
            "step 20400 | loss 1.8012 | step time 148.18ms\n",
            "step 20400 train loss: 1.7058 test loss: 1.8103\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20425 | loss 1.7168 | step time 148.05ms\n",
            "step 20450 | loss 1.7178 | step time 148.16ms\n",
            "step 20475 | loss 1.7257 | step time 148.26ms\n",
            "step 20500 | loss 1.7563 | step time 148.01ms\n",
            "step 20525 | loss 1.7064 | step time 147.96ms\n",
            "step 20550 | loss 1.7399 | step time 148.09ms\n",
            "step 20575 | loss 1.7526 | step time 148.21ms\n",
            "step 20600 | loss 1.6605 | step time 148.10ms\n",
            "step 20600 train loss: 1.7107 test loss: 1.8139\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20625 | loss 1.6645 | step time 148.06ms\n",
            "step 20650 | loss 1.7966 | step time 148.18ms\n",
            "step 20675 | loss 1.7232 | step time 147.91ms\n",
            "step 20700 | loss 1.7041 | step time 148.02ms\n",
            "step 20725 | loss 1.6944 | step time 148.05ms\n",
            "step 20750 | loss 1.6735 | step time 148.35ms\n",
            "step 20775 | loss 1.7282 | step time 148.02ms\n",
            "step 20800 | loss 1.7674 | step time 148.19ms\n",
            "step 20800 train loss: 1.7376 test loss: 1.8100\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20825 | loss 1.7385 | step time 147.98ms\n",
            "step 20850 | loss 1.6750 | step time 148.29ms\n",
            "step 20875 | loss 1.7161 | step time 148.15ms\n",
            "step 20900 | loss 1.7149 | step time 148.02ms\n",
            "step 20925 | loss 1.7087 | step time 148.16ms\n",
            "step 20950 | loss 1.7158 | step time 148.05ms\n",
            "step 20975 | loss 1.7458 | step time 148.14ms\n",
            "step 21000 | loss 1.6976 | step time 148.30ms\n",
            "step 21000 train loss: 1.7205 test loss: 1.8148\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 21025 | loss 1.7731 | step time 148.22ms\n",
            "step 21050 | loss 1.7157 | step time 147.86ms\n",
            "step 21075 | loss 1.6882 | step time 147.98ms\n",
            "step 21100 | loss 1.7297 | step time 147.85ms\n",
            "step 21125 | loss 1.7301 | step time 148.55ms\n",
            "step 21150 | loss 1.7946 | step time 148.00ms\n",
            "step 21175 | loss 1.7540 | step time 148.01ms\n",
            "step 21200 | loss 1.7899 | step time 148.15ms\n",
            "step 21200 train loss: 1.7268 test loss: 1.8105\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 21225 | loss 1.7451 | step time 147.98ms\n",
            "step 21250 | loss 1.7216 | step time 147.98ms\n",
            "step 21275 | loss 1.7080 | step time 147.87ms\n",
            "step 21300 | loss 1.7336 | step time 148.16ms\n",
            "step 21325 | loss 1.7672 | step time 148.55ms\n",
            "step 21350 | loss 1.7838 | step time 147.98ms\n",
            "step 21375 | loss 1.7321 | step time 148.07ms\n",
            "step 21400 | loss 1.6977 | step time 148.33ms\n",
            "step 21400 train loss: 1.7156 test loss: 1.8151\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 21425 | loss 1.7280 | step time 147.87ms\n",
            "step 21450 | loss 1.6659 | step time 148.14ms\n",
            "step 21475 | loss 1.6936 | step time 147.98ms\n",
            "step 21500 | loss 1.7200 | step time 148.09ms\n",
            "step 21525 | loss 1.7512 | step time 148.12ms\n",
            "step 21550 | loss 1.7334 | step time 148.00ms\n",
            "step 21575 | loss 1.7000 | step time 148.00ms\n",
            "step 21600 | loss 1.6945 | step time 148.38ms\n",
            "step 21600 train loss: 1.7177 test loss: 1.8121\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 21625 | loss 1.6454 | step time 147.98ms\n",
            "step 21650 | loss 1.7129 | step time 147.83ms\n",
            "step 21675 | loss 1.7912 | step time 147.95ms\n",
            "step 21700 | loss 1.6913 | step time 147.85ms\n",
            "step 21725 | loss 1.6908 | step time 147.95ms\n",
            "step 21750 | loss 1.7784 | step time 148.11ms\n",
            "step 21775 | loss 1.7315 | step time 148.29ms\n",
            "step 21800 | loss 1.7348 | step time 148.30ms\n",
            "step 21800 train loss: 1.7275 test loss: 1.7973\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 21825 | loss 1.7680 | step time 148.31ms\n",
            "step 21850 | loss 1.7487 | step time 148.00ms\n",
            "step 21875 | loss 1.6685 | step time 147.93ms\n",
            "step 21900 | loss 1.7470 | step time 147.81ms\n",
            "step 21925 | loss 1.6289 | step time 148.04ms\n",
            "step 21950 | loss 1.7275 | step time 148.04ms\n",
            "step 21975 | loss 1.6713 | step time 147.94ms\n",
            "step 22000 | loss 1.7588 | step time 148.39ms\n",
            "step 22000 train loss: 1.7408 test loss: 1.8091\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22025 | loss 1.6901 | step time 148.12ms\n",
            "step 22050 | loss 1.7359 | step time 147.92ms\n",
            "step 22075 | loss 1.6694 | step time 147.91ms\n",
            "step 22100 | loss 1.6682 | step time 147.95ms\n",
            "step 22125 | loss 1.7277 | step time 148.24ms\n",
            "step 22150 | loss 1.7106 | step time 148.18ms\n",
            "step 22175 | loss 1.6755 | step time 148.03ms\n",
            "step 22200 | loss 1.6880 | step time 148.41ms\n",
            "step 22200 train loss: 1.7222 test loss: 1.8120\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22225 | loss 1.7011 | step time 148.48ms\n",
            "step 22250 | loss 1.7543 | step time 147.77ms\n",
            "step 22275 | loss 1.7633 | step time 147.96ms\n",
            "step 22300 | loss 1.6713 | step time 147.92ms\n",
            "step 22325 | loss 1.7245 | step time 148.14ms\n",
            "step 22350 | loss 1.7341 | step time 147.97ms\n",
            "step 22375 | loss 1.7275 | step time 148.05ms\n",
            "step 22400 | loss 1.7594 | step time 148.50ms\n",
            "step 22400 train loss: 1.7238 test loss: 1.8078\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22425 | loss 1.6915 | step time 148.02ms\n",
            "step 22450 | loss 1.7598 | step time 147.87ms\n",
            "step 22475 | loss 1.7448 | step time 147.97ms\n",
            "step 22500 | loss 1.7488 | step time 147.97ms\n",
            "step 22525 | loss 1.7393 | step time 147.83ms\n",
            "step 22550 | loss 1.6990 | step time 148.78ms\n",
            "step 22575 | loss 1.6989 | step time 148.03ms\n",
            "step 22600 | loss 1.6782 | step time 148.46ms\n",
            "step 22600 train loss: 1.7355 test loss: 1.8166\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22625 | loss 1.6492 | step time 148.01ms\n",
            "step 22650 | loss 1.7598 | step time 147.87ms\n",
            "step 22675 | loss 1.7450 | step time 148.12ms\n",
            "step 22700 | loss 1.7317 | step time 147.86ms\n",
            "step 22725 | loss 1.7516 | step time 148.09ms\n",
            "step 22750 | loss 1.7193 | step time 147.98ms\n",
            "step 22775 | loss 1.6720 | step time 148.06ms\n",
            "step 22800 | loss 1.7833 | step time 148.52ms\n",
            "step 22800 train loss: 1.7234 test loss: 1.8124\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22825 | loss 1.7992 | step time 147.85ms\n",
            "step 22850 | loss 1.7293 | step time 147.86ms\n",
            "step 22875 | loss 1.6630 | step time 147.94ms\n",
            "step 22900 | loss 1.6666 | step time 148.19ms\n",
            "step 22925 | loss 1.7424 | step time 148.01ms\n",
            "step 22950 | loss 1.7628 | step time 147.92ms\n",
            "step 22975 | loss 1.6504 | step time 148.00ms\n",
            "step 23000 | loss 1.7227 | step time 148.35ms\n",
            "step 23000 train loss: 1.7189 test loss: 1.8247\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 23025 | loss 1.6954 | step time 148.07ms\n",
            "step 23050 | loss 1.7544 | step time 147.87ms\n",
            "step 23075 | loss 1.6979 | step time 147.89ms\n",
            "step 23100 | loss 1.6577 | step time 147.93ms\n",
            "step 23125 | loss 1.7383 | step time 147.93ms\n",
            "step 23150 | loss 1.7197 | step time 148.24ms\n",
            "step 23175 | loss 1.7197 | step time 147.91ms\n",
            "step 23200 | loss 1.7213 | step time 148.67ms\n",
            "step 23200 train loss: 1.7225 test loss: 1.8106\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 23225 | loss 1.6981 | step time 148.17ms\n",
            "step 23250 | loss 1.7023 | step time 147.87ms\n",
            "step 23275 | loss 1.7463 | step time 147.85ms\n",
            "step 23300 | loss 1.6973 | step time 147.81ms\n",
            "step 23325 | loss 1.7569 | step time 147.82ms\n",
            "step 23350 | loss 1.7114 | step time 148.00ms\n",
            "step 23375 | loss 1.7239 | step time 148.05ms\n",
            "step 23400 | loss 1.7909 | step time 148.29ms\n",
            "step 23400 train loss: 1.7115 test loss: 1.8100\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 23425 | loss 1.7633 | step time 148.11ms\n",
            "step 23450 | loss 1.7157 | step time 148.05ms\n",
            "step 23475 | loss 1.6806 | step time 148.00ms\n",
            "step 23500 | loss 1.7148 | step time 148.01ms\n",
            "step 23525 | loss 1.7403 | step time 148.17ms\n",
            "step 23550 | loss 1.7333 | step time 147.90ms\n",
            "step 23575 | loss 1.7487 | step time 147.93ms\n",
            "step 23600 | loss 1.7785 | step time 148.38ms\n",
            "step 23600 train loss: 1.7146 test loss: 1.8127\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 23625 | loss 1.6732 | step time 148.49ms\n",
            "step 23650 | loss 1.7699 | step time 148.05ms\n",
            "step 23675 | loss 1.7482 | step time 147.92ms\n",
            "step 23700 | loss 1.6859 | step time 148.08ms\n",
            "step 23725 | loss 1.7333 | step time 147.93ms\n",
            "step 23750 | loss 1.7304 | step time 148.03ms\n",
            "step 23775 | loss 1.7379 | step time 147.94ms\n",
            "step 23800 | loss 1.7154 | step time 148.50ms\n",
            "step 23800 train loss: 1.7159 test loss: 1.8176\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 23825 | loss 1.7448 | step time 148.21ms\n",
            "step 23850 | loss 1.6793 | step time 147.85ms\n",
            "step 23875 | loss 1.7283 | step time 147.81ms\n",
            "step 23900 | loss 1.6935 | step time 147.95ms\n",
            "step 23925 | loss 1.6873 | step time 147.97ms\n",
            "step 23950 | loss 1.7338 | step time 148.03ms\n",
            "step 23975 | loss 1.6768 | step time 148.12ms\n",
            "step 24000 | loss 1.7359 | step time 148.54ms\n",
            "step 24000 train loss: 1.7162 test loss: 1.8150\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24025 | loss 1.7284 | step time 148.22ms\n",
            "step 24050 | loss 1.7724 | step time 147.80ms\n",
            "step 24075 | loss 1.7383 | step time 147.91ms\n",
            "step 24100 | loss 1.7326 | step time 148.07ms\n",
            "step 24125 | loss 1.6806 | step time 148.00ms\n",
            "step 24150 | loss 1.7015 | step time 147.99ms\n",
            "step 24175 | loss 1.7069 | step time 148.12ms\n",
            "step 24200 | loss 1.6579 | step time 148.06ms\n",
            "step 24200 train loss: 1.7082 test loss: 1.8099\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24225 | loss 1.6896 | step time 148.00ms\n",
            "step 24250 | loss 1.7385 | step time 147.97ms\n",
            "step 24275 | loss 1.6885 | step time 147.94ms\n",
            "step 24300 | loss 1.7187 | step time 148.51ms\n",
            "step 24325 | loss 1.7053 | step time 147.97ms\n",
            "step 24350 | loss 1.6992 | step time 147.98ms\n",
            "step 24375 | loss 1.7330 | step time 148.16ms\n",
            "step 24400 | loss 1.7267 | step time 147.96ms\n",
            "step 24400 train loss: 1.7197 test loss: 1.8206\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24425 | loss 1.7062 | step time 148.23ms\n",
            "step 24450 | loss 1.7096 | step time 147.93ms\n",
            "step 24475 | loss 1.7477 | step time 147.90ms\n",
            "step 24500 | loss 1.7229 | step time 148.22ms\n",
            "step 24525 | loss 1.7334 | step time 147.86ms\n",
            "step 24550 | loss 1.6960 | step time 164.74ms\n",
            "step 24575 | loss 1.7336 | step time 147.85ms\n",
            "step 24600 | loss 1.7056 | step time 147.96ms\n",
            "step 24600 train loss: 1.7144 test loss: 1.8127\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24625 | loss 1.6642 | step time 148.11ms\n",
            "step 24650 | loss 1.7494 | step time 147.78ms\n",
            "step 24675 | loss 1.6431 | step time 147.86ms\n",
            "step 24700 | loss 1.6622 | step time 148.10ms\n",
            "step 24725 | loss 1.7167 | step time 148.05ms\n",
            "step 24750 | loss 1.7581 | step time 148.06ms\n",
            "step 24775 | loss 1.6878 | step time 148.02ms\n",
            "step 24800 | loss 1.6710 | step time 148.41ms\n",
            "step 24800 train loss: 1.7291 test loss: 1.8144\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24825 | loss 1.7547 | step time 148.04ms\n",
            "step 24850 | loss 1.6957 | step time 147.87ms\n",
            "step 24875 | loss 1.7374 | step time 147.98ms\n",
            "step 24900 | loss 1.7668 | step time 148.03ms\n",
            "step 24925 | loss 1.7648 | step time 147.98ms\n",
            "step 24950 | loss 1.7364 | step time 147.78ms\n",
            "step 24975 | loss 1.6933 | step time 147.96ms\n",
            "step 25000 | loss 1.7371 | step time 148.13ms\n",
            "step 25000 train loss: 1.7139 test loss: 1.8193\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 25025 | loss 1.7469 | step time 148.23ms\n",
            "step 25050 | loss 1.6933 | step time 147.97ms\n",
            "step 25075 | loss 1.7222 | step time 147.99ms\n",
            "step 25100 | loss 1.6889 | step time 148.08ms\n",
            "step 25125 | loss 1.6705 | step time 147.86ms\n",
            "step 25150 | loss 1.7330 | step time 148.00ms\n",
            "step 25175 | loss 1.7667 | step time 148.09ms\n",
            "step 25200 | loss 1.7259 | step time 148.12ms\n",
            "step 25200 train loss: 1.7236 test loss: 1.8159\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 25225 | loss 1.6673 | step time 148.04ms\n",
            "step 25250 | loss 1.7335 | step time 147.82ms\n",
            "step 25275 | loss 1.7520 | step time 147.84ms\n",
            "step 25300 | loss 1.7873 | step time 148.26ms\n",
            "step 25325 | loss 1.7191 | step time 147.74ms\n",
            "step 25350 | loss 1.7219 | step time 148.10ms\n",
            "step 25375 | loss 1.7035 | step time 147.99ms\n",
            "step 25400 | loss 1.7124 | step time 148.08ms\n",
            "step 25400 train loss: 1.6957 test loss: 1.8165\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 25425 | loss 1.6932 | step time 148.19ms\n",
            "step 25450 | loss 1.7387 | step time 147.91ms\n",
            "step 25475 | loss 1.6307 | step time 148.04ms\n",
            "step 25500 | loss 1.6459 | step time 148.02ms\n",
            "step 25525 | loss 1.7154 | step time 148.13ms\n",
            "step 25550 | loss 1.6568 | step time 148.00ms\n",
            "step 25575 | loss 1.7379 | step time 148.17ms\n",
            "step 25600 | loss 1.7676 | step time 148.14ms\n",
            "step 25600 train loss: 1.7163 test loss: 1.8260\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "q83i143gIwwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "9ce8fade90e349b58530887b4f9d8466",
            "622f0937c8ee45d28237c599e643036d",
            "a6fc04b6810b4a8589ecd9c102449e97",
            "4812536113004e28894b7429e1d2ba18",
            "d8ca28b4cad3494788003cc319d281a5",
            "c8cd2f3e2738497096f5511f50b409b9",
            "99eec3cdaa30452ebdc23e3b3d499914",
            "b50dff2dff4e48f6be1967df6fb3aeab"
          ]
        },
        "outputId": "258f058a-581b-4090-c1f0-f1ecb52188d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.836 MB of 0.836 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ce8fade90e349b58530887b4f9d8466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>1200</td></tr><tr><td>test_loss</td><td>3.58529</td></tr><tr><td>train_loss</td><td>2.22445</td></tr><tr><td>train_loss_step</td><td>2.21137</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">0706_0035_ct_223samples_4char+</strong> at: <a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B/runs/096jzfkd' target=\"_blank\">https://wandb.ai/sam-greydanus/ct_223samples_4char%2B/runs/096jzfkd</a><br/> View project at: <a href='https://wandb.ai/sam-greydanus/ct_223samples_4char%2B' target=\"_blank\">https://wandb.ai/sam-greydanus/ct_223samples_4char%2B</a><br/>Synced 5 W&B file(s), 69 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240706_003635-096jzfkd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=200)\n",
        "img = mpimg.imread('test_sample_1.png')\n",
        "plt.imshow(img) ; plt.axis('off') ; plt.show()"
      ],
      "metadata": {
        "id": "CtO814TOsWU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}